{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.vgg_cifar import MaskVGG,VGG\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import os\n",
    "from lib.utils import AverageMeter,accuracy\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vgg = VGG('vgg16').cuda()\n",
    "vgg.load_state_dict(torch.load('C:\\\\Users\\\\lenovo\\\\Desktop\\\\cacp\\\\amc_vgg\\\\checkpoints\\\\vgg16_cifar10.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_dataset(dset_name, batch_size, n_worker, val_size, data_root='../data',\n",
    "                      use_real_val=False, shuffle=True):\n",
    "    '''\n",
    "        split the train set into train / val for rl search\n",
    "    '''\n",
    "    if shuffle:\n",
    "        index_sampler = SubsetRandomSampler\n",
    "    else:  # every time we use the same order for the split subset\n",
    "        class SubsetSequentialSampler(SubsetRandomSampler):\n",
    "            def __iter__(self):\n",
    "                return (self.indices[i] for i in torch.arange(len(self.indices)).int())\n",
    "        index_sampler = SubsetSequentialSampler\n",
    "\n",
    "    print('=> Preparing data: {}...'.format(dset_name))\n",
    "    if dset_name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_train)\n",
    "        if use_real_val:  # split the actual val set\n",
    "            valset = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform_test)\n",
    "            n_val = len(valset)\n",
    "            assert val_size < n_val\n",
    "            indices = list(range(n_val))\n",
    "            np.random.shuffle(indices)\n",
    "            _, val_idx = indices[val_size:], indices[:val_size]\n",
    "            train_idx = list(range(len(trainset)))  # all train set for train\n",
    "        else:  # split the train set\n",
    "            valset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_test)\n",
    "            n_train = len(trainset)\n",
    "            indices = list(range(n_train))\n",
    "            # now shuffle the indices\n",
    "            np.random.shuffle(indices)\n",
    "            assert val_size < n_train\n",
    "            train_idx, val_idx = indices[val_size:], indices[:val_size]\n",
    "\n",
    "        train_sampler = index_sampler(train_idx)\n",
    "        val_sampler = index_sampler(val_idx)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, sampler=train_sampler,\n",
    "                                                   num_workers=n_worker, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, sampler=val_sampler,\n",
    "                                                 num_workers=n_worker, pin_memory=True)\n",
    "        n_class = 10\n",
    "        \n",
    "    elif dset_name == 'imagenet':\n",
    "        train_dir = os.path.join(data_root, 'train')\n",
    "        val_dir = os.path.join(data_root, 'val')\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        input_size = 224\n",
    "        train_transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "        test_transform = transforms.Compose([\n",
    "                transforms.Resize(int(input_size/0.875)),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "        trainset = datasets.ImageFolder(train_dir, train_transform)\n",
    "        if use_real_val:\n",
    "            valset = datasets.ImageFolder(val_dir, test_transform)\n",
    "            n_val = len(valset)\n",
    "            assert val_size < n_val\n",
    "            indices = list(range(n_val))\n",
    "            np.random.shuffle(indices)\n",
    "            _, val_idx = indices[val_size:], indices[:val_size]\n",
    "            train_idx = list(range(len(trainset)))  # all trainset\n",
    "        else:\n",
    "            valset = datasets.ImageFolder(train_dir, test_transform)\n",
    "            n_train = len(trainset)\n",
    "            indices = list(range(n_train))\n",
    "            np.random.shuffle(indices)\n",
    "            assert val_size < n_train\n",
    "            train_idx, val_idx = indices[val_size:], indices[:val_size]\n",
    "\n",
    "        train_sampler = index_sampler(train_idx)\n",
    "        val_sampler = index_sampler(val_idx)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                                   num_workers=n_worker, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, sampler=val_sampler,\n",
    "                                                 num_workers=n_worker, pin_memory=True)\n",
    "\n",
    "        n_class = 1000\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return train_loader, val_loader, n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(val_loader, model, verbose=True):\n",
    "        '''\n",
    "        Validate the performance on validation set\n",
    "        :param val_loader:\n",
    "        :param model:\n",
    "        :param verbose:\n",
    "        :return:\n",
    "        '''\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        top5 = AverageMeter()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "        end = time.time()\n",
    "\n",
    "        t1 = time.time()\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(val_loader):\n",
    "                target = target.cuda(non_blocking=True)\n",
    "                input_var = torch.autograd.Variable(input).cuda()\n",
    "                target_var = torch.autograd.Variable(target).cuda()\n",
    "\n",
    "                # compute output\n",
    "                output = model(input_var)\n",
    "                loss = criterion(output, target_var)\n",
    "\n",
    "                # measure accuracy and record loss\n",
    "                prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "                losses.update(loss.item(), input.size(0))\n",
    "                top1.update(prec1.item(), input.size(0))\n",
    "                top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "                # measure elapsed time\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "        t2 = time.time()\n",
    "        if verbose:\n",
    "            print('* Test loss: %.3f    top1: %.3f    top5: %.3f    time: %.3f' %\n",
    "                  (losses.avg, top1.avg, top5.avg, t2 - t1))\n",
    "        # if self.acc_metric == 'acc1':\n",
    "        #     return top1.avg\n",
    "        # elif self.acc_metric == 'acc5':\n",
    "        #     return top5.avg\n",
    "        # else:\n",
    "        #     raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Preparing data: cifar10...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "* Test loss: 0.296    top1: 93.180    top5: 99.820    time: 4.259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, val_loader, n_class = get_split_dataset('cifar10', 50,\n",
    "                                                                        0, 5000,\n",
    "                                                                        data_root='C:\\\\Users\\\\lenovo\\\\dataset\\\\cifar',\n",
    "                                                                        use_real_val=True,\n",
    "                                                                        shuffle=False)  # same sampling\n",
    "_validate(val_loader,vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_masked(vgg_origin,strategy):\n",
    "    from models.vgg_cifar import MaskVGG\n",
    "    masked_vgg = MaskVGG('vgg16',strategy)\n",
    "    \n",
    "    orimo_ls = list(vgg_origin.modules())\n",
    "    \n",
    "    for i,m in enumerate(masked_vgg.modules()):\n",
    "        if type(m) in [nn.Conv2d,nn.BatchNorm2d,nn.Linear,nn.AvgPool2d,nn.MaxPool2d,nn.ReLU]:\n",
    "            # type\n",
    "            ty = type(m)\n",
    "\n",
    "            # conv\n",
    "            if ty == nn.Conv2d:\n",
    "                m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "                m.bias.data.copy_(orimo_ls[i].bias.data)\n",
    "            # bn2d\n",
    "            elif ty == nn.BatchNorm2d:\n",
    "                m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "                m.bias.data.copy_(orimo_ls[i].bias.data)\n",
    "                m.running_mean.data.copy_(orimo_ls[i].running_mean.data)\n",
    "                m.running_var.data.copy_(orimo_ls[i].running_var.data)\n",
    "            elif ty == nn.Linear:\n",
    "                # linear\n",
    "                m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "            else:# maxpool,avgpool,relu don't need params\n",
    "                pass\n",
    "    masked_vgg = masked_vgg.cuda()\n",
    "    return masked_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Test loss: 0.296    top1: 93.180    top5: 99.800    time: 2.665\n"
     ]
    }
   ],
   "source": [
    "mask = vgg_masked(vgg,strategy=[1.0]*13)\n",
    "_validate(val_loader,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idx = []\n",
    "all_ops= []\n",
    "prunable_idx = []\n",
    "prunable_ops = []\n",
    "orgin_channel = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,m in enumerate(vgg.modules()):\n",
    "    if type(m) in [nn.Conv2d,nn.BatchNorm2d,nn.Linear,nn.AvgPool2d,nn.MaxPool2d,nn.ReLU]:\n",
    "        all_idx.append(i)\n",
    "        all_ops.append(m)\n",
    "        if type(m) in [nn.Conv2d]:\n",
    "            prunable_idx.append(i)\n",
    "            prunable_ops.append(m)\n",
    "            orgin_channel.append(m.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2idxx(idx):\n",
    "    return prunable_idx[idx]\n",
    "def idxx2idx(idxx):\n",
    "    return prunable_idx.index(idxx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_get_mask(select,method = 'l1'):\n",
    "    mask = []\n",
    "    for i,a in enumerate(select):\n",
    "        c = orgin_channel[i]\n",
    "        d = int(c * a)\n",
    "        mask_ = np.zeros(c,bool)\n",
    "        weight = prunable_ops[i].weight.data.cpu().numpy()\n",
    "        if method == 'l1':\n",
    "            importance = np.abs(weight).sum((1, 2, 3))\n",
    "            sorted_idx = np.argsort(-importance)  # sum magnitude along C_in, sort descend\n",
    "            preserve_idx = sorted_idx[:d]  # to preserve index\n",
    "            mask_[preserve_idx] = True\n",
    "        mask.append(mask_)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = [0.5]*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prunable_idx = []\n",
    "prunable_ops = []\n",
    "layer_type_dict = {}\n",
    "org_channels = {}\n",
    "conv_buffer_dict = {} # layer after the conv\n",
    "all_idx = []\n",
    "buffer_conv_map = {}\n",
    "\n",
    "i=0\n",
    "buffer_temp_idx = []\n",
    "modules = list(vgg.modules())\n",
    "n = len(modules)\n",
    "while i < n :\n",
    "    m = modules[i]\n",
    "\n",
    "    if type(m) not in [nn.Conv2d,nn.BatchNorm2d,nn.Linear,nn.AvgPool2d,nn.MaxPool2d,nn.ReLU]:\n",
    "        i+=1\n",
    "        continue\n",
    "    else:\n",
    "        assert type(m) == torch.nn.modules.conv.Conv2d\n",
    "        prunable_ops.append(m)\n",
    "        prunable_idx.append(i)\n",
    "        layer_type_dict[i] = type(m)\n",
    "        org_channels[i] = (m.in_channels) \n",
    "        \n",
    "        buffer_temp_idx = []\n",
    "        while i != n-1:\n",
    "            i+=1\n",
    "            bu = modules[i]\n",
    "            if type(bu) is torch.nn.modules.conv.Conv2d:\n",
    "                i-=1\n",
    "                break\n",
    "            buffer_temp_idx.append(i)\n",
    "        conv_buffer_dict[prunable_idx[-1]] = copy.deepcopy(buffer_temp_idx)\n",
    "        for j in buffer_temp_idx:\n",
    "            buffer_conv_map[j] = prunable_idx[-1]\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruned_model(origin_model,pruned_model,all_mask):\n",
    "    m_list = list(origin_model.modules())\n",
    "    mp_list = list(pruned_model.modules())\n",
    "    st = time.time()\n",
    "    for idx,idxx in enumerate(prunable_idx):\n",
    "        \n",
    "        # replace conv first\n",
    "        mask = all_mask[idx]\n",
    "        weight = m_list[idxx].weight.data.cpu().numpy()\n",
    "        bias = m_list[idxx].bias.data.cpu().numpy()\n",
    "        \n",
    "        mask_weight = None\n",
    "        if idx == 0:\n",
    "            mask_weight = weight[mask,:,:,:]\n",
    "        else:\n",
    "            input_mask = all_mask[idx-1]\n",
    "            # select input\n",
    "            mask_weight = weight[:,input_mask,:,:].reshape(weight.shape[0],-1,weight.shape[2],weight.shape[3])\n",
    "            # select output\n",
    "            mask_weight = mask_weight[mask,:,:,:].reshape(-1,mask_weight.shape[1],mask_weight.shape[2],mask_weight.shape[3])\n",
    "        mask_bias = bias[mask]\n",
    "        mp = mp_list[idxx]\n",
    "        mp.weight.data.copy_(torch.from_numpy(mask_weight).cuda())\n",
    "        mp.bias.data.copy_(torch.from_numpy(mask_bias).cuda())\n",
    "\n",
    "        # replace other layers\n",
    "        buffer = conv_buffer_dict[idxx]\n",
    "        for buffer_idx in buffer:\n",
    "            m = m_list[buffer_idx]\n",
    "            mp = mp_list[buffer_idx]\n",
    "            if type(m) == nn.BatchNorm2d:\n",
    "                mp.weight.data.copy_(torch.from_numpy(m.weight.data.cpu().numpy()[mask]).cuda())\n",
    "                mp.bias.data.copy_(torch.from_numpy(m.bias.data.cpu().numpy()[mask]).cuda())\n",
    "                mp.running_mean.data.copy_(torch.from_numpy(m.running_mean.data.cpu().numpy()[mask]).cuda())\n",
    "                mp.running_var.data.copy_(torch.from_numpy(m.running_var.data.cpu().numpy()[mask]).cuda())\n",
    "            elif type(m) == nn.Linear:\n",
    "                mp.weight.data.copy_(torch.from_numpy(m.weight.data.cpu().numpy()[:,mask]).cuda())\n",
    "    ed = time.time()\n",
    "    print(f'replace cost {ed-st}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Test loss: 2.303    top1: 9.860    top5: 49.980    time: 2.647\n",
      "replace cost 0.19254016876220703s\n",
      "* Test loss: 0.737    top1: 83.040    top5: 98.120    time: 2.541\n"
     ]
    }
   ],
   "source": [
    "select = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "mask = preprocess_get_mask(select)\n",
    "# mask = vgg_masked(vgg,strategy=select)\n",
    "from models.vgg_cifar import MaskVGG\n",
    "masked_vgg = MaskVGG('vgg16',select).cuda()\n",
    "_validate(val_loader,masked_vgg)\n",
    "pruned_model(vgg,masked_vgg,mask)\n",
    "_validate(val_loader,masked_vgg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
