{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.vgg_cifar import MaskVGG,VGG\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import os\n",
    "from lib.utils import AverageMeter,accuracy\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from model.googlenet import Inception\n",
    "# from utils.options import args\n",
    "# import utils.common as utils\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import heapq\n",
    "# from data import cifar10, cifar100, imagenet\n",
    "from importlib import import_module\n",
    "\n",
    "device = torch.device(f\"cuda:0\") if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "\n",
    "vgg = VGG('vgg16').cuda()\n",
    "vgg.load_state_dict(torch.load('C:\\\\Users\\\\lenovo\\\\Desktop\\\\cacp\\\\amc_vgg\\\\checkpoints\\\\vgg16_cifar10.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "list(vgg.modules())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,c,H,W = list(vgg.modules())[2].weight.size()\n",
    "# c\n",
    "# H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_dataset(dset_name, batch_size, n_worker, val_size, data_root='../data',\n",
    "                      use_real_val=False, shuffle=True):\n",
    "    '''\n",
    "        split the train set into train / val for rl search\n",
    "    '''\n",
    "    if shuffle:\n",
    "        index_sampler = SubsetRandomSampler\n",
    "    else:  # every time we use the same order for the split subset\n",
    "        class SubsetSequentialSampler(SubsetRandomSampler):\n",
    "            def __iter__(self):\n",
    "                return (self.indices[i] for i in torch.arange(len(self.indices)).int())\n",
    "        index_sampler = SubsetSequentialSampler\n",
    "\n",
    "    print('=> Preparing data: {}...'.format(dset_name))\n",
    "    if dset_name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_train)\n",
    "        if use_real_val:  # split the actual val set\n",
    "            valset = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform_test)\n",
    "            n_val = len(valset)\n",
    "            assert val_size < n_val\n",
    "            indices = list(range(n_val))\n",
    "            np.random.shuffle(indices)\n",
    "            _, val_idx = indices[val_size:], indices[:val_size]\n",
    "            train_idx = list(range(len(trainset)))  # all train set for train\n",
    "        else:  # split the train set\n",
    "            valset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_test)\n",
    "            n_train = len(trainset)\n",
    "            indices = list(range(n_train))\n",
    "            # now shuffle the indices\n",
    "            np.random.shuffle(indices)\n",
    "            assert val_size < n_train\n",
    "            train_idx, val_idx = indices[val_size:], indices[:val_size]\n",
    "\n",
    "        train_sampler = index_sampler(train_idx)\n",
    "        val_sampler = index_sampler(val_idx)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, sampler=train_sampler,\n",
    "                                                   num_workers=n_worker, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, sampler=val_sampler,\n",
    "                                                 num_workers=n_worker, pin_memory=True)\n",
    "        n_class = 10\n",
    "        \n",
    "    elif dset_name == 'imagenet':\n",
    "        train_dir = os.path.join(data_root, 'train')\n",
    "        val_dir = os.path.join(data_root, 'val')\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        input_size = 224\n",
    "        train_transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "        test_transform = transforms.Compose([\n",
    "                transforms.Resize(int(input_size/0.875)),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "        trainset = datasets.ImageFolder(train_dir, train_transform)\n",
    "        if use_real_val:\n",
    "            valset = datasets.ImageFolder(val_dir, test_transform)\n",
    "            n_val = len(valset)\n",
    "            assert val_size < n_val\n",
    "            indices = list(range(n_val))\n",
    "            np.random.shuffle(indices)\n",
    "            _, val_idx = indices[val_size:], indices[:val_size]\n",
    "            train_idx = list(range(len(trainset)))  # all trainset\n",
    "        else:\n",
    "            valset = datasets.ImageFolder(train_dir, test_transform)\n",
    "            n_train = len(trainset)\n",
    "            indices = list(range(n_train))\n",
    "            np.random.shuffle(indices)\n",
    "            assert val_size < n_train\n",
    "            train_idx, val_idx = indices[val_size:], indices[:val_size]\n",
    "\n",
    "        train_sampler = index_sampler(train_idx)\n",
    "        val_sampler = index_sampler(val_idx)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                                   num_workers=n_worker, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, sampler=val_sampler,\n",
    "                                                 num_workers=n_worker, pin_memory=True)\n",
    "\n",
    "        n_class = 1000\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return train_loader, val_loader, n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(val_loader, model, verbose=True):\n",
    "        '''\n",
    "        Validate the performance on validation set\n",
    "        :param val_loader:\n",
    "        :param model:\n",
    "        :param verbose:\n",
    "        :return:\n",
    "        '''\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        top5 = AverageMeter()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "        end = time.time()\n",
    "\n",
    "        t1 = time.time()\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(val_loader):\n",
    "                target = target.cuda(non_blocking=True)\n",
    "                input_var = torch.autograd.Variable(input).cuda()\n",
    "                target_var = torch.autograd.Variable(target).cuda()\n",
    "\n",
    "                # compute output\n",
    "                output = model(input_var)\n",
    "                loss = criterion(output, target_var)\n",
    "\n",
    "                # measure accuracy and record loss\n",
    "                prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "                losses.update(loss.item(), input.size(0))\n",
    "                top1.update(prec1.item(), input.size(0))\n",
    "                top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "                # measure elapsed time\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "        t2 = time.time()\n",
    "        if verbose:\n",
    "            print('* Test loss: %.3f    top1: %.3f    top5: %.3f    time: %.3f' %\n",
    "                  (losses.avg, top1.avg, top5.avg, t2 - t1))\n",
    "        # if self.acc_metric == 'acc1':\n",
    "        #     return top1.avg\n",
    "        # elif self.acc_metric == 'acc5':\n",
    "        #     return top5.avg\n",
    "        # else:\n",
    "        #     raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Preparing data: cifar10...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "* Test loss: 0.317    top1: 92.920    top5: 99.740    time: 4.354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, val_loader, n_class = get_split_dataset('cifar10', 50,\n",
    "                                                                        0, 5000,\n",
    "                                                                        data_root='C:\\\\Users\\\\lenovo\\\\dataset\\\\cifar',\n",
    "                                                                        use_real_val=True,\n",
    "                                                                        shuffle=False)  # same sampling\n",
    "_validate(val_loader,vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vgg_masked(vgg_origin,strategy):\n",
    "#     from models.vgg_cifar import MaskVGG\n",
    "#     masked_vgg = MaskVGG('vgg16',strategy)\n",
    "    \n",
    "#     orimo_ls = list(vgg_origin.modules())\n",
    "    \n",
    "#     for i,m in enumerate(masked_vgg.modules()):\n",
    "#         if type(m) in [nn.Conv2d,nn.BatchNorm2d,nn.Linear,nn.AvgPool2d,nn.MaxPool2d,nn.ReLU]:\n",
    "#             # type\n",
    "#             ty = type(m)\n",
    "\n",
    "#             # conv\n",
    "#             if ty == nn.Conv2d:\n",
    "#                 m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "#                 m.bias.data.copy_(orimo_ls[i].bias.data)\n",
    "#             # bn2d\n",
    "#             elif ty == nn.BatchNorm2d:\n",
    "#                 m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "#                 m.bias.data.copy_(orimo_ls[i].bias.data)\n",
    "#                 m.running_mean.data.copy_(orimo_ls[i].running_mean.data)\n",
    "#                 m.running_var.data.copy_(orimo_ls[i].running_var.data)\n",
    "#             elif ty == nn.Linear:\n",
    "#                 # linear\n",
    "#                 m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "#             else:# maxpool,avgpool,relu don't need params\n",
    "#                 pass\n",
    "#     masked_vgg = masked_vgg.cuda()\n",
    "#     return masked_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = vgg_masked(vgg,strategy=[1.0]*13)\n",
    "# _validate(val_loader,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idx = []\n",
    "all_ops= []\n",
    "prunable_idx = []\n",
    "prunable_ops = []\n",
    "orgin_channel = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,m in enumerate(vgg.modules()):\n",
    "    if type(m) in [nn.Conv2d,nn.BatchNorm2d,nn.Linear,nn.AvgPool2d,nn.MaxPool2d,nn.ReLU]:\n",
    "        all_idx.append(i)\n",
    "        all_ops.append(m)\n",
    "        if type(m) in [nn.Conv2d]:\n",
    "            prunable_idx.append(i)\n",
    "            prunable_ops.append(m)\n",
    "            orgin_channel.append(m.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2idxx(idx):\n",
    "    return prunable_idx[idx]\n",
    "def idxx2idx(idxx):\n",
    "    return prunable_idx.index(idxx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_get_mask(select,method = 'l1'):\n",
    "    mask = []\n",
    "    for i,a in enumerate(select):\n",
    "        c = orgin_channel[i]\n",
    "        d = int(c * a)\n",
    "        mask_ = np.zeros(c,bool)\n",
    "        weight = prunable_ops[i].weight.data.cpu().numpy()\n",
    "        if method == 'l1':\n",
    "            importance = np.abs(weight).sum((1, 2, 3))\n",
    "            sorted_idx = np.argsort(-importance)  # sum magnitude along C_in, sort descend\n",
    "            preserve_idx = sorted_idx[:d]  # to preserve index\n",
    "            mask_[preserve_idx] = True\n",
    "        mask.append(mask_)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prunable_idx = []\n",
    "prunable_ops = []\n",
    "layer_type_dict = {}\n",
    "org_channels = {}\n",
    "conv_buffer_dict = {} # layer after the conv\n",
    "all_idx = []\n",
    "buffer_conv_map = {}\n",
    "\n",
    "i=0\n",
    "buffer_temp_idx = []\n",
    "modules = list(vgg.modules())\n",
    "n = len(modules)\n",
    "while i < n :\n",
    "    m = modules[i]\n",
    "\n",
    "    if type(m) not in [nn.Conv2d,nn.BatchNorm2d,nn.Linear,nn.AvgPool2d,nn.MaxPool2d,nn.ReLU]:\n",
    "        i+=1\n",
    "        continue\n",
    "    else:\n",
    "        assert type(m) == torch.nn.modules.conv.Conv2d\n",
    "        prunable_ops.append(m)\n",
    "        prunable_idx.append(i)\n",
    "        layer_type_dict[i] = type(m)\n",
    "        org_channels[i] = (m.in_channels) \n",
    "        \n",
    "        buffer_temp_idx = []\n",
    "        while i != n-1:\n",
    "            i+=1\n",
    "            bu = modules[i]\n",
    "            if type(bu) is torch.nn.modules.conv.Conv2d:\n",
    "                i-=1\n",
    "                break\n",
    "            buffer_temp_idx.append(i)\n",
    "        conv_buffer_dict[prunable_idx[-1]] = copy.deepcopy(buffer_temp_idx)\n",
    "        for j in buffer_temp_idx:\n",
    "            buffer_conv_map[j] = prunable_idx[-1]\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruned_model(origin_model,pruned_model,all_mask):\n",
    "    m_list = list(origin_model.modules())\n",
    "    mp_list = list(pruned_model.modules())\n",
    "    st = time.time()\n",
    "    for idx,idxx in enumerate(prunable_idx):\n",
    "        \n",
    "        # replace conv first\n",
    "        mask = all_mask[idx]\n",
    "        weight = m_list[idxx].weight.data.cpu().numpy()\n",
    "        bias = m_list[idxx].bias.data.cpu().numpy()\n",
    "        \n",
    "        mask_weight = None\n",
    "        if idx == 0:\n",
    "            mask_weight = weight[mask,:,:,:]\n",
    "        else:\n",
    "            input_mask = all_mask[idx-1]\n",
    "            # select input\n",
    "            mask_weight = weight[:,input_mask,:,:].reshape(weight.shape[0],-1,weight.shape[2],weight.shape[3])\n",
    "            # select output\n",
    "            mask_weight = mask_weight[mask,:,:,:].reshape(-1,mask_weight.shape[1],mask_weight.shape[2],mask_weight.shape[3])\n",
    "        mask_bias = bias[mask]\n",
    "        mp = mp_list[idxx]\n",
    "        mp.weight.data.copy_(torch.from_numpy(mask_weight).cuda())\n",
    "        mp.bias.data.copy_(torch.from_numpy(mask_bias).cuda())\n",
    "\n",
    "        # replace other layers\n",
    "        buffer = conv_buffer_dict[idxx]\n",
    "        for buffer_idx in buffer:\n",
    "            m = m_list[buffer_idx]\n",
    "            mp = mp_list[buffer_idx]\n",
    "            if type(m) == nn.BatchNorm2d:\n",
    "                mp.weight.data.copy_(torch.from_numpy(m.weight.data.cpu().numpy()[mask]).cuda())\n",
    "                mp.bias.data.copy_(torch.from_numpy(m.bias.data.cpu().numpy()[mask]).cuda())\n",
    "                mp.running_mean.data.copy_(torch.from_numpy(m.running_mean.data.cpu().numpy()[mask]).cuda())\n",
    "                mp.running_var.data.copy_(torch.from_numpy(m.running_var.data.cpu().numpy()[mask]).cuda())\n",
    "            elif type(m) == nn.Linear:\n",
    "                mp.weight.data.copy_(torch.from_numpy(m.weight.data.cpu().numpy()[:,mask]).cuda())\n",
    "    ed = time.time()\n",
    "#     print(f'replace cost {ed-st}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_vgg(select):\n",
    "    mask = preprocess_get_mask(select)\n",
    "    from models.vgg_cifar import MaskVGG\n",
    "    masked_vgg = MaskVGG('vgg16',select).cuda()\n",
    "    pruned_model(vgg,masked_vgg,mask)\n",
    "    return masked_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bf_af_acc(select):\n",
    "    mask = preprocess_get_mask(select)\n",
    "    from models.vgg_cifar import MaskVGG\n",
    "    masked_vgg = MaskVGG('vgg16',select).cuda()\n",
    "    _validate(val_loader,masked_vgg)\n",
    "    pruned_model(vgg,masked_vgg,mask)\n",
    "    _validate(val_loader,masked_vgg)\n",
    "\n",
    "# select = [0.75, 0.375, 0.8125, 1.0, 0.8125, 0.53125, 0.71875, 0.875, 0.296875, 0.59375, 0.921875, 0.265625, 0.203125]\n",
    "# select = [0.7]*13\n",
    "# select = [0.25, 0.5, 0.5625, 0.25, 0.3125, 0.21875, 0.21875, 0.203125, 0.203125, 0.59375, 0.203125, 0.203125, 0.546875]\n",
    "# get_bf_af_acc(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#args:gpu data_path train_batch_size eval_batch_size\n",
    "class Data:\n",
    "    def __init__(self, args):\n",
    "        if args['gpus'] is not None:\n",
    "            pin_memory = True\n",
    "\n",
    "\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        trainset = CIFAR10(root=args['data_path'], train=True, download= False, transform=transform_train)\n",
    "\n",
    "        self.trainLoader = DataLoader(\n",
    "            trainset, batch_size=args['train_batch_size'], shuffle=True,\n",
    "            num_workers=args['num_workers'], pin_memory=pin_memory\n",
    "        )\n",
    "\n",
    "        testset = CIFAR10(root=args['data_path'], train=False, download= False, transform=transform_test)\n",
    "        self.testLoader = DataLoader(\n",
    "            testset, batch_size=args['eval_batch_size'], shuffle=False,\n",
    "            num_workers=args['num_workers'], pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading Data..\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Loading Data..')\n",
    "args = {}\n",
    "args['data_path'] = 'C:\\\\Users\\\\lenovo\\\\dataset\\\\cifar'\n",
    "args['train_batch_size'] = 256\n",
    "args['num_workers'] = 0\n",
    "args['eval_batch_size'] = 256\n",
    "args['gpus'] = 0\n",
    "# args.gpus\n",
    "loader = Data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        if self.count > 0:\n",
    "            self.avg = self.sum / self.count\n",
    "\n",
    "    def accumulate(self, val, n=1):\n",
    "        self.sum += val\n",
    "        self.count += n\n",
    "        if self.count > 0:\n",
    "            self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    batch_size = target.size(0)\n",
    "    num = output.size(1)\n",
    "    target_topk = []\n",
    "    appendices = []\n",
    "    for k in topk:\n",
    "        if k <= num:\n",
    "            target_topk.append(k)\n",
    "        else:\n",
    "            appendices.append([0.0])\n",
    "    topk = target_topk\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res + appendices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "def train(model, optimizer, trainLoader, args, epoch):\n",
    "\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    accurary = AverageMeter()\n",
    "    print_freq = len(trainLoader.dataset) // args['train_batch_size'] // 10\n",
    "    start_time = time.time()\n",
    "    for batch, (inputs, targets) in enumerate(trainLoader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_func(output, targets)\n",
    "        loss.backward()\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        optimizer.step()\n",
    "\n",
    "        prec1 = accuracy(output, targets)\n",
    "        accurary.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        if batch % print_freq == 0 and batch != 0:\n",
    "            current_time = time.time()\n",
    "            cost_time = current_time - start_time\n",
    "            logger.info(\n",
    "                'Epoch[{}] ({}/{}):\\t'\n",
    "                'Loss {:.4f}\\t'\n",
    "                'Accurary {:.2f}%\\t\\t'\n",
    "                'Time {:.2f}s'.format(\n",
    "                    epoch, batch * args['train_batch_size'], len(trainLoader.dataset),\n",
    "                    float(losses.avg), float(accurary.avg), cost_time\n",
    "                )\n",
    "            )\n",
    "            start_time = current_time\n",
    "            \n",
    "\n",
    "#Testinga\n",
    "def test(model, testLoader):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    accurary = AverageMeter()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testLoader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, targets)\n",
    "\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            predicted = accuracy(outputs, targets)\n",
    "            accurary.update(predicted[0], inputs.size(0))\n",
    "\n",
    "        current_time = time.time()\n",
    "        logger.info(\n",
    "            'Test Loss {:.4f}\\tAccurary {:.2f}%\\t\\tTime {:.2f}s\\n'\n",
    "            .format(float(losses.avg), float(accurary.avg), (current_time - start_time))\n",
    "        )\n",
    "    return accurary.avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         model_state_dict = model.module.state_dict() if len(args.gpus) > 1 else model.state_dict()\n",
    "    \n",
    "#         state = {\n",
    "#             'state_dict': model_state_dict,\n",
    "#             'best_acc': best_acc,\n",
    "#             'optimizer': optimizer.state_dict(),\n",
    "#             'scheduler': scheduler.state_dict(),\n",
    "#             'epoch': epoch + 1,\n",
    "#             'action': ai\n",
    "#         }\n",
    "#         checkpoint.save_model(state, epoch + 1, is_best\n",
    "#         ckpt_dir = './checkpoints'\n",
    "#         save_path = f'{ckpt_dir}/model_{epoch}.pt'\n",
    "#         # print('=> Saving model to {}'.format(save_path))\n",
    "#         torch.save(state, save_path)\n",
    "#         if is_best:\n",
    "#             shutil.copyfile(save_path, f'{self.ckpt_dir}/model_best.pt')\n",
    "\n",
    "#     logger.info('Best accurary: {:.3f}'.format(float(best_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(file_path):\n",
    "    import logging \n",
    "    logger = logging.getLogger('gal')\n",
    "    log_format = '%(asctime)s | %(message)s'\n",
    "    formatter = logging.Formatter(log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "    file_handler = logging.FileHandler(file_path)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bf_acc 正比于 ft_acc吗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/21 06:39:03 PM | Epoch[0] (4864/50000):\tLoss 0.4436\tAccurary 86.43%\t\tTime 24.78s\n",
      "01/21 06:39:03 PM | Epoch[0] (4864/50000):\tLoss 0.4436\tAccurary 86.43%\t\tTime 24.78s\n",
      "01/21 06:39:03 PM | Epoch[0] (4864/50000):\tLoss 0.4436\tAccurary 86.43%\t\tTime 24.78s\n",
      "01/21 06:39:03 PM | Epoch[0] (4864/50000):\tLoss 0.4436\tAccurary 86.43%\t\tTime 24.78s\n",
      "01/21 06:39:03 PM | Epoch[0] (4864/50000):\tLoss 0.4436\tAccurary 86.43%\t\tTime 24.78s\n",
      "01/21 06:39:22 PM | Epoch[0] (9728/50000):\tLoss 0.3625\tAccurary 88.49%\t\tTime 19.91s\n",
      "01/21 06:39:22 PM | Epoch[0] (9728/50000):\tLoss 0.3625\tAccurary 88.49%\t\tTime 19.91s\n",
      "01/21 06:39:22 PM | Epoch[0] (9728/50000):\tLoss 0.3625\tAccurary 88.49%\t\tTime 19.91s\n",
      "01/21 06:39:22 PM | Epoch[0] (9728/50000):\tLoss 0.3625\tAccurary 88.49%\t\tTime 19.91s\n",
      "01/21 06:39:22 PM | Epoch[0] (9728/50000):\tLoss 0.3625\tAccurary 88.49%\t\tTime 19.91s\n",
      "01/21 06:39:42 PM | Epoch[0] (14592/50000):\tLoss 0.3107\tAccurary 90.10%\t\tTime 20.03s\n",
      "01/21 06:39:42 PM | Epoch[0] (14592/50000):\tLoss 0.3107\tAccurary 90.10%\t\tTime 20.03s\n",
      "01/21 06:39:42 PM | Epoch[0] (14592/50000):\tLoss 0.3107\tAccurary 90.10%\t\tTime 20.03s\n",
      "01/21 06:39:42 PM | Epoch[0] (14592/50000):\tLoss 0.3107\tAccurary 90.10%\t\tTime 20.03s\n",
      "01/21 06:39:42 PM | Epoch[0] (14592/50000):\tLoss 0.3107\tAccurary 90.10%\t\tTime 20.03s\n",
      "01/21 06:40:03 PM | Epoch[0] (19456/50000):\tLoss 0.2804\tAccurary 90.98%\t\tTime 20.26s\n",
      "01/21 06:40:03 PM | Epoch[0] (19456/50000):\tLoss 0.2804\tAccurary 90.98%\t\tTime 20.26s\n",
      "01/21 06:40:03 PM | Epoch[0] (19456/50000):\tLoss 0.2804\tAccurary 90.98%\t\tTime 20.26s\n",
      "01/21 06:40:03 PM | Epoch[0] (19456/50000):\tLoss 0.2804\tAccurary 90.98%\t\tTime 20.26s\n",
      "01/21 06:40:03 PM | Epoch[0] (19456/50000):\tLoss 0.2804\tAccurary 90.98%\t\tTime 20.26s\n",
      "01/21 06:40:23 PM | Epoch[0] (24320/50000):\tLoss 0.2574\tAccurary 91.69%\t\tTime 20.49s\n",
      "01/21 06:40:23 PM | Epoch[0] (24320/50000):\tLoss 0.2574\tAccurary 91.69%\t\tTime 20.49s\n",
      "01/21 06:40:23 PM | Epoch[0] (24320/50000):\tLoss 0.2574\tAccurary 91.69%\t\tTime 20.49s\n",
      "01/21 06:40:23 PM | Epoch[0] (24320/50000):\tLoss 0.2574\tAccurary 91.69%\t\tTime 20.49s\n",
      "01/21 06:40:23 PM | Epoch[0] (24320/50000):\tLoss 0.2574\tAccurary 91.69%\t\tTime 20.49s\n",
      "01/21 06:40:43 PM | Epoch[0] (29184/50000):\tLoss 0.2434\tAccurary 92.09%\t\tTime 19.87s\n",
      "01/21 06:40:43 PM | Epoch[0] (29184/50000):\tLoss 0.2434\tAccurary 92.09%\t\tTime 19.87s\n",
      "01/21 06:40:43 PM | Epoch[0] (29184/50000):\tLoss 0.2434\tAccurary 92.09%\t\tTime 19.87s\n",
      "01/21 06:40:43 PM | Epoch[0] (29184/50000):\tLoss 0.2434\tAccurary 92.09%\t\tTime 19.87s\n",
      "01/21 06:40:43 PM | Epoch[0] (29184/50000):\tLoss 0.2434\tAccurary 92.09%\t\tTime 19.87s\n",
      "01/21 06:41:03 PM | Epoch[0] (34048/50000):\tLoss 0.2326\tAccurary 92.40%\t\tTime 19.98s\n",
      "01/21 06:41:03 PM | Epoch[0] (34048/50000):\tLoss 0.2326\tAccurary 92.40%\t\tTime 19.98s\n",
      "01/21 06:41:03 PM | Epoch[0] (34048/50000):\tLoss 0.2326\tAccurary 92.40%\t\tTime 19.98s\n",
      "01/21 06:41:03 PM | Epoch[0] (34048/50000):\tLoss 0.2326\tAccurary 92.40%\t\tTime 19.98s\n",
      "01/21 06:41:03 PM | Epoch[0] (34048/50000):\tLoss 0.2326\tAccurary 92.40%\t\tTime 19.98s\n",
      "01/21 06:41:23 PM | Epoch[0] (38912/50000):\tLoss 0.2267\tAccurary 92.60%\t\tTime 19.83s\n",
      "01/21 06:41:23 PM | Epoch[0] (38912/50000):\tLoss 0.2267\tAccurary 92.60%\t\tTime 19.83s\n",
      "01/21 06:41:23 PM | Epoch[0] (38912/50000):\tLoss 0.2267\tAccurary 92.60%\t\tTime 19.83s\n",
      "01/21 06:41:23 PM | Epoch[0] (38912/50000):\tLoss 0.2267\tAccurary 92.60%\t\tTime 19.83s\n",
      "01/21 06:41:23 PM | Epoch[0] (38912/50000):\tLoss 0.2267\tAccurary 92.60%\t\tTime 19.83s\n",
      "01/21 06:41:43 PM | Epoch[0] (43776/50000):\tLoss 0.2207\tAccurary 92.77%\t\tTime 20.47s\n",
      "01/21 06:41:43 PM | Epoch[0] (43776/50000):\tLoss 0.2207\tAccurary 92.77%\t\tTime 20.47s\n",
      "01/21 06:41:43 PM | Epoch[0] (43776/50000):\tLoss 0.2207\tAccurary 92.77%\t\tTime 20.47s\n",
      "01/21 06:41:43 PM | Epoch[0] (43776/50000):\tLoss 0.2207\tAccurary 92.77%\t\tTime 20.47s\n",
      "01/21 06:41:43 PM | Epoch[0] (43776/50000):\tLoss 0.2207\tAccurary 92.77%\t\tTime 20.47s\n",
      "01/21 06:42:03 PM | Epoch[0] (48640/50000):\tLoss 0.2170\tAccurary 92.85%\t\tTime 20.05s\n",
      "01/21 06:42:03 PM | Epoch[0] (48640/50000):\tLoss 0.2170\tAccurary 92.85%\t\tTime 20.05s\n",
      "01/21 06:42:03 PM | Epoch[0] (48640/50000):\tLoss 0.2170\tAccurary 92.85%\t\tTime 20.05s\n",
      "01/21 06:42:03 PM | Epoch[0] (48640/50000):\tLoss 0.2170\tAccurary 92.85%\t\tTime 20.05s\n",
      "01/21 06:42:03 PM | Epoch[0] (48640/50000):\tLoss 0.2170\tAccurary 92.85%\t\tTime 20.05s\n",
      "01/21 06:42:13 PM | Test Loss 0.4028\tAccurary 87.03%\t\tTime 4.64s\n",
      "\n",
      "01/21 06:42:13 PM | Test Loss 0.4028\tAccurary 87.03%\t\tTime 4.64s\n",
      "\n",
      "01/21 06:42:13 PM | Test Loss 0.4028\tAccurary 87.03%\t\tTime 4.64s\n",
      "\n",
      "01/21 06:42:13 PM | Test Loss 0.4028\tAccurary 87.03%\t\tTime 4.64s\n",
      "\n",
      "01/21 06:42:13 PM | Test Loss 0.4028\tAccurary 87.03%\t\tTime 4.64s\n",
      "\n",
      "01/21 06:42:38 PM | Epoch[1] (4864/50000):\tLoss 0.1807\tAccurary 93.89%\t\tTime 25.43s\n",
      "01/21 06:42:38 PM | Epoch[1] (4864/50000):\tLoss 0.1807\tAccurary 93.89%\t\tTime 25.43s\n",
      "01/21 06:42:38 PM | Epoch[1] (4864/50000):\tLoss 0.1807\tAccurary 93.89%\t\tTime 25.43s\n",
      "01/21 06:42:38 PM | Epoch[1] (4864/50000):\tLoss 0.1807\tAccurary 93.89%\t\tTime 25.43s\n",
      "01/21 06:42:38 PM | Epoch[1] (4864/50000):\tLoss 0.1807\tAccurary 93.89%\t\tTime 25.43s\n",
      "01/21 06:42:58 PM | Epoch[1] (9728/50000):\tLoss 0.1610\tAccurary 94.68%\t\tTime 19.81s\n",
      "01/21 06:42:58 PM | Epoch[1] (9728/50000):\tLoss 0.1610\tAccurary 94.68%\t\tTime 19.81s\n",
      "01/21 06:42:58 PM | Epoch[1] (9728/50000):\tLoss 0.1610\tAccurary 94.68%\t\tTime 19.81s\n",
      "01/21 06:42:58 PM | Epoch[1] (9728/50000):\tLoss 0.1610\tAccurary 94.68%\t\tTime 19.81s\n",
      "01/21 06:42:58 PM | Epoch[1] (9728/50000):\tLoss 0.1610\tAccurary 94.68%\t\tTime 19.81s\n",
      "01/21 06:43:18 PM | Epoch[1] (14592/50000):\tLoss 0.1554\tAccurary 94.79%\t\tTime 19.87s\n",
      "01/21 06:43:18 PM | Epoch[1] (14592/50000):\tLoss 0.1554\tAccurary 94.79%\t\tTime 19.87s\n",
      "01/21 06:43:18 PM | Epoch[1] (14592/50000):\tLoss 0.1554\tAccurary 94.79%\t\tTime 19.87s\n",
      "01/21 06:43:18 PM | Epoch[1] (14592/50000):\tLoss 0.1554\tAccurary 94.79%\t\tTime 19.87s\n",
      "01/21 06:43:18 PM | Epoch[1] (14592/50000):\tLoss 0.1554\tAccurary 94.79%\t\tTime 19.87s\n",
      "01/21 06:43:38 PM | Epoch[1] (19456/50000):\tLoss 0.1592\tAccurary 94.59%\t\tTime 20.81s\n",
      "01/21 06:43:38 PM | Epoch[1] (19456/50000):\tLoss 0.1592\tAccurary 94.59%\t\tTime 20.81s\n",
      "01/21 06:43:38 PM | Epoch[1] (19456/50000):\tLoss 0.1592\tAccurary 94.59%\t\tTime 20.81s\n",
      "01/21 06:43:38 PM | Epoch[1] (19456/50000):\tLoss 0.1592\tAccurary 94.59%\t\tTime 20.81s\n",
      "01/21 06:43:38 PM | Epoch[1] (19456/50000):\tLoss 0.1592\tAccurary 94.59%\t\tTime 20.81s\n",
      "01/21 06:43:59 PM | Epoch[1] (24320/50000):\tLoss 0.1657\tAccurary 94.35%\t\tTime 20.49s\n",
      "01/21 06:43:59 PM | Epoch[1] (24320/50000):\tLoss 0.1657\tAccurary 94.35%\t\tTime 20.49s\n",
      "01/21 06:43:59 PM | Epoch[1] (24320/50000):\tLoss 0.1657\tAccurary 94.35%\t\tTime 20.49s\n",
      "01/21 06:43:59 PM | Epoch[1] (24320/50000):\tLoss 0.1657\tAccurary 94.35%\t\tTime 20.49s\n",
      "01/21 06:43:59 PM | Epoch[1] (24320/50000):\tLoss 0.1657\tAccurary 94.35%\t\tTime 20.49s\n",
      "01/21 06:44:20 PM | Epoch[1] (29184/50000):\tLoss 0.1673\tAccurary 94.27%\t\tTime 20.66s\n",
      "01/21 06:44:20 PM | Epoch[1] (29184/50000):\tLoss 0.1673\tAccurary 94.27%\t\tTime 20.66s\n",
      "01/21 06:44:20 PM | Epoch[1] (29184/50000):\tLoss 0.1673\tAccurary 94.27%\t\tTime 20.66s\n",
      "01/21 06:44:20 PM | Epoch[1] (29184/50000):\tLoss 0.1673\tAccurary 94.27%\t\tTime 20.66s\n",
      "01/21 06:44:20 PM | Epoch[1] (29184/50000):\tLoss 0.1673\tAccurary 94.27%\t\tTime 20.66s\n",
      "01/21 06:44:41 PM | Epoch[1] (34048/50000):\tLoss 0.1684\tAccurary 94.23%\t\tTime 21.34s\n",
      "01/21 06:44:41 PM | Epoch[1] (34048/50000):\tLoss 0.1684\tAccurary 94.23%\t\tTime 21.34s\n",
      "01/21 06:44:41 PM | Epoch[1] (34048/50000):\tLoss 0.1684\tAccurary 94.23%\t\tTime 21.34s\n",
      "01/21 06:44:41 PM | Epoch[1] (34048/50000):\tLoss 0.1684\tAccurary 94.23%\t\tTime 21.34s\n",
      "01/21 06:44:41 PM | Epoch[1] (34048/50000):\tLoss 0.1684\tAccurary 94.23%\t\tTime 21.34s\n",
      "01/21 06:45:02 PM | Epoch[1] (38912/50000):\tLoss 0.1692\tAccurary 94.22%\t\tTime 21.06s\n",
      "01/21 06:45:02 PM | Epoch[1] (38912/50000):\tLoss 0.1692\tAccurary 94.22%\t\tTime 21.06s\n",
      "01/21 06:45:02 PM | Epoch[1] (38912/50000):\tLoss 0.1692\tAccurary 94.22%\t\tTime 21.06s\n",
      "01/21 06:45:02 PM | Epoch[1] (38912/50000):\tLoss 0.1692\tAccurary 94.22%\t\tTime 21.06s\n",
      "01/21 06:45:02 PM | Epoch[1] (38912/50000):\tLoss 0.1692\tAccurary 94.22%\t\tTime 21.06s\n",
      "01/21 06:45:24 PM | Epoch[1] (43776/50000):\tLoss 0.1675\tAccurary 94.27%\t\tTime 21.52s\n",
      "01/21 06:45:24 PM | Epoch[1] (43776/50000):\tLoss 0.1675\tAccurary 94.27%\t\tTime 21.52s\n",
      "01/21 06:45:24 PM | Epoch[1] (43776/50000):\tLoss 0.1675\tAccurary 94.27%\t\tTime 21.52s\n",
      "01/21 06:45:24 PM | Epoch[1] (43776/50000):\tLoss 0.1675\tAccurary 94.27%\t\tTime 21.52s\n",
      "01/21 06:45:24 PM | Epoch[1] (43776/50000):\tLoss 0.1675\tAccurary 94.27%\t\tTime 21.52s\n",
      "01/21 06:45:44 PM | Epoch[1] (48640/50000):\tLoss 0.1705\tAccurary 94.21%\t\tTime 20.83s\n",
      "01/21 06:45:44 PM | Epoch[1] (48640/50000):\tLoss 0.1705\tAccurary 94.21%\t\tTime 20.83s\n",
      "01/21 06:45:44 PM | Epoch[1] (48640/50000):\tLoss 0.1705\tAccurary 94.21%\t\tTime 20.83s\n",
      "01/21 06:45:44 PM | Epoch[1] (48640/50000):\tLoss 0.1705\tAccurary 94.21%\t\tTime 20.83s\n",
      "01/21 06:45:44 PM | Epoch[1] (48640/50000):\tLoss 0.1705\tAccurary 94.21%\t\tTime 20.83s\n",
      "01/21 06:45:54 PM | Test Loss 0.4237\tAccurary 87.09%\t\tTime 4.98s\n",
      "\n",
      "01/21 06:45:54 PM | Test Loss 0.4237\tAccurary 87.09%\t\tTime 4.98s\n",
      "\n",
      "01/21 06:45:54 PM | Test Loss 0.4237\tAccurary 87.09%\t\tTime 4.98s\n",
      "\n",
      "01/21 06:45:54 PM | Test Loss 0.4237\tAccurary 87.09%\t\tTime 4.98s\n",
      "\n",
      "01/21 06:45:54 PM | Test Loss 0.4237\tAccurary 87.09%\t\tTime 4.98s\n",
      "\n",
      "01/21 06:45:54 PM | ############# model0_2epoch:87.08999633789062 using 436.34533643722534s #################\n",
      "01/21 06:45:54 PM | ############# model0_2epoch:87.08999633789062 using 436.34533643722534s #################\n",
      "01/21 06:45:54 PM | ############# model0_2epoch:87.08999633789062 using 436.34533643722534s #################\n",
      "01/21 06:45:54 PM | ############# model0_2epoch:87.08999633789062 using 436.34533643722534s #################\n",
      "01/21 06:45:54 PM | ############# model0_2epoch:87.08999633789062 using 436.34533643722534s #################\n",
      "01/21 06:46:16 PM | Epoch[0] (4864/50000):\tLoss 0.4871\tAccurary 85.51%\t\tTime 21.60s\n",
      "01/21 06:46:16 PM | Epoch[0] (4864/50000):\tLoss 0.4871\tAccurary 85.51%\t\tTime 21.60s\n",
      "01/21 06:46:16 PM | Epoch[0] (4864/50000):\tLoss 0.4871\tAccurary 85.51%\t\tTime 21.60s\n",
      "01/21 06:46:16 PM | Epoch[0] (4864/50000):\tLoss 0.4871\tAccurary 85.51%\t\tTime 21.60s\n",
      "01/21 06:46:16 PM | Epoch[0] (4864/50000):\tLoss 0.4871\tAccurary 85.51%\t\tTime 21.60s\n",
      "01/21 06:46:32 PM | Epoch[0] (9728/50000):\tLoss 0.3904\tAccurary 87.85%\t\tTime 16.39s\n",
      "01/21 06:46:32 PM | Epoch[0] (9728/50000):\tLoss 0.3904\tAccurary 87.85%\t\tTime 16.39s\n",
      "01/21 06:46:32 PM | Epoch[0] (9728/50000):\tLoss 0.3904\tAccurary 87.85%\t\tTime 16.39s\n",
      "01/21 06:46:32 PM | Epoch[0] (9728/50000):\tLoss 0.3904\tAccurary 87.85%\t\tTime 16.39s\n",
      "01/21 06:46:32 PM | Epoch[0] (9728/50000):\tLoss 0.3904\tAccurary 87.85%\t\tTime 16.39s\n",
      "01/21 06:46:49 PM | Epoch[0] (14592/50000):\tLoss 0.3334\tAccurary 89.41%\t\tTime 16.63s\n",
      "01/21 06:46:49 PM | Epoch[0] (14592/50000):\tLoss 0.3334\tAccurary 89.41%\t\tTime 16.63s\n",
      "01/21 06:46:49 PM | Epoch[0] (14592/50000):\tLoss 0.3334\tAccurary 89.41%\t\tTime 16.63s\n",
      "01/21 06:46:49 PM | Epoch[0] (14592/50000):\tLoss 0.3334\tAccurary 89.41%\t\tTime 16.63s\n",
      "01/21 06:46:49 PM | Epoch[0] (14592/50000):\tLoss 0.3334\tAccurary 89.41%\t\tTime 16.63s\n",
      "01/21 06:47:06 PM | Epoch[0] (19456/50000):\tLoss 0.3004\tAccurary 90.44%\t\tTime 16.73s\n",
      "01/21 06:47:06 PM | Epoch[0] (19456/50000):\tLoss 0.3004\tAccurary 90.44%\t\tTime 16.73s\n",
      "01/21 06:47:06 PM | Epoch[0] (19456/50000):\tLoss 0.3004\tAccurary 90.44%\t\tTime 16.73s\n",
      "01/21 06:47:06 PM | Epoch[0] (19456/50000):\tLoss 0.3004\tAccurary 90.44%\t\tTime 16.73s\n",
      "01/21 06:47:06 PM | Epoch[0] (19456/50000):\tLoss 0.3004\tAccurary 90.44%\t\tTime 16.73s\n",
      "01/21 06:47:23 PM | Epoch[0] (24320/50000):\tLoss 0.2741\tAccurary 91.20%\t\tTime 16.86s\n",
      "01/21 06:47:23 PM | Epoch[0] (24320/50000):\tLoss 0.2741\tAccurary 91.20%\t\tTime 16.86s\n",
      "01/21 06:47:23 PM | Epoch[0] (24320/50000):\tLoss 0.2741\tAccurary 91.20%\t\tTime 16.86s\n",
      "01/21 06:47:23 PM | Epoch[0] (24320/50000):\tLoss 0.2741\tAccurary 91.20%\t\tTime 16.86s\n",
      "01/21 06:47:23 PM | Epoch[0] (24320/50000):\tLoss 0.2741\tAccurary 91.20%\t\tTime 16.86s\n",
      "01/21 06:47:39 PM | Epoch[0] (29184/50000):\tLoss 0.2582\tAccurary 91.65%\t\tTime 16.42s\n",
      "01/21 06:47:39 PM | Epoch[0] (29184/50000):\tLoss 0.2582\tAccurary 91.65%\t\tTime 16.42s\n",
      "01/21 06:47:39 PM | Epoch[0] (29184/50000):\tLoss 0.2582\tAccurary 91.65%\t\tTime 16.42s\n",
      "01/21 06:47:39 PM | Epoch[0] (29184/50000):\tLoss 0.2582\tAccurary 91.65%\t\tTime 16.42s\n",
      "01/21 06:47:39 PM | Epoch[0] (29184/50000):\tLoss 0.2582\tAccurary 91.65%\t\tTime 16.42s\n",
      "01/21 06:47:55 PM | Epoch[0] (34048/50000):\tLoss 0.2480\tAccurary 91.90%\t\tTime 16.29s\n",
      "01/21 06:47:55 PM | Epoch[0] (34048/50000):\tLoss 0.2480\tAccurary 91.90%\t\tTime 16.29s\n",
      "01/21 06:47:55 PM | Epoch[0] (34048/50000):\tLoss 0.2480\tAccurary 91.90%\t\tTime 16.29s\n",
      "01/21 06:47:55 PM | Epoch[0] (34048/50000):\tLoss 0.2480\tAccurary 91.90%\t\tTime 16.29s\n",
      "01/21 06:47:55 PM | Epoch[0] (34048/50000):\tLoss 0.2480\tAccurary 91.90%\t\tTime 16.29s\n",
      "01/21 06:48:12 PM | Epoch[0] (38912/50000):\tLoss 0.2414\tAccurary 92.09%\t\tTime 16.34s\n",
      "01/21 06:48:12 PM | Epoch[0] (38912/50000):\tLoss 0.2414\tAccurary 92.09%\t\tTime 16.34s\n",
      "01/21 06:48:12 PM | Epoch[0] (38912/50000):\tLoss 0.2414\tAccurary 92.09%\t\tTime 16.34s\n",
      "01/21 06:48:12 PM | Epoch[0] (38912/50000):\tLoss 0.2414\tAccurary 92.09%\t\tTime 16.34s\n",
      "01/21 06:48:12 PM | Epoch[0] (38912/50000):\tLoss 0.2414\tAccurary 92.09%\t\tTime 16.34s\n",
      "01/21 06:48:28 PM | Epoch[0] (43776/50000):\tLoss 0.2346\tAccurary 92.28%\t\tTime 16.09s\n",
      "01/21 06:48:28 PM | Epoch[0] (43776/50000):\tLoss 0.2346\tAccurary 92.28%\t\tTime 16.09s\n",
      "01/21 06:48:28 PM | Epoch[0] (43776/50000):\tLoss 0.2346\tAccurary 92.28%\t\tTime 16.09s\n",
      "01/21 06:48:28 PM | Epoch[0] (43776/50000):\tLoss 0.2346\tAccurary 92.28%\t\tTime 16.09s\n",
      "01/21 06:48:28 PM | Epoch[0] (43776/50000):\tLoss 0.2346\tAccurary 92.28%\t\tTime 16.09s\n",
      "01/21 06:48:44 PM | Epoch[0] (48640/50000):\tLoss 0.2290\tAccurary 92.45%\t\tTime 16.01s\n",
      "01/21 06:48:44 PM | Epoch[0] (48640/50000):\tLoss 0.2290\tAccurary 92.45%\t\tTime 16.01s\n",
      "01/21 06:48:44 PM | Epoch[0] (48640/50000):\tLoss 0.2290\tAccurary 92.45%\t\tTime 16.01s\n",
      "01/21 06:48:44 PM | Epoch[0] (48640/50000):\tLoss 0.2290\tAccurary 92.45%\t\tTime 16.01s\n",
      "01/21 06:48:44 PM | Epoch[0] (48640/50000):\tLoss 0.2290\tAccurary 92.45%\t\tTime 16.01s\n",
      "01/21 06:48:52 PM | Test Loss 0.4080\tAccurary 86.86%\t\tTime 4.50s\n",
      "\n",
      "01/21 06:48:52 PM | Test Loss 0.4080\tAccurary 86.86%\t\tTime 4.50s\n",
      "\n",
      "01/21 06:48:52 PM | Test Loss 0.4080\tAccurary 86.86%\t\tTime 4.50s\n",
      "\n",
      "01/21 06:48:52 PM | Test Loss 0.4080\tAccurary 86.86%\t\tTime 4.50s\n",
      "\n",
      "01/21 06:48:52 PM | Test Loss 0.4080\tAccurary 86.86%\t\tTime 4.50s\n",
      "\n",
      "01/21 06:49:13 PM | Epoch[1] (4864/50000):\tLoss 0.1534\tAccurary 94.90%\t\tTime 20.57s\n",
      "01/21 06:49:13 PM | Epoch[1] (4864/50000):\tLoss 0.1534\tAccurary 94.90%\t\tTime 20.57s\n",
      "01/21 06:49:13 PM | Epoch[1] (4864/50000):\tLoss 0.1534\tAccurary 94.90%\t\tTime 20.57s\n",
      "01/21 06:49:13 PM | Epoch[1] (4864/50000):\tLoss 0.1534\tAccurary 94.90%\t\tTime 20.57s\n",
      "01/21 06:49:13 PM | Epoch[1] (4864/50000):\tLoss 0.1534\tAccurary 94.90%\t\tTime 20.57s\n",
      "01/21 06:49:29 PM | Epoch[1] (9728/50000):\tLoss 0.1567\tAccurary 94.94%\t\tTime 16.38s\n",
      "01/21 06:49:29 PM | Epoch[1] (9728/50000):\tLoss 0.1567\tAccurary 94.94%\t\tTime 16.38s\n",
      "01/21 06:49:29 PM | Epoch[1] (9728/50000):\tLoss 0.1567\tAccurary 94.94%\t\tTime 16.38s\n",
      "01/21 06:49:29 PM | Epoch[1] (9728/50000):\tLoss 0.1567\tAccurary 94.94%\t\tTime 16.38s\n",
      "01/21 06:49:29 PM | Epoch[1] (9728/50000):\tLoss 0.1567\tAccurary 94.94%\t\tTime 16.38s\n",
      "01/21 06:49:45 PM | Epoch[1] (14592/50000):\tLoss 0.1534\tAccurary 95.02%\t\tTime 16.26s\n",
      "01/21 06:49:45 PM | Epoch[1] (14592/50000):\tLoss 0.1534\tAccurary 95.02%\t\tTime 16.26s\n",
      "01/21 06:49:45 PM | Epoch[1] (14592/50000):\tLoss 0.1534\tAccurary 95.02%\t\tTime 16.26s\n",
      "01/21 06:49:45 PM | Epoch[1] (14592/50000):\tLoss 0.1534\tAccurary 95.02%\t\tTime 16.26s\n",
      "01/21 06:49:45 PM | Epoch[1] (14592/50000):\tLoss 0.1534\tAccurary 95.02%\t\tTime 16.26s\n",
      "01/21 06:50:02 PM | Epoch[1] (19456/50000):\tLoss 0.1535\tAccurary 94.97%\t\tTime 16.59s\n",
      "01/21 06:50:02 PM | Epoch[1] (19456/50000):\tLoss 0.1535\tAccurary 94.97%\t\tTime 16.59s\n",
      "01/21 06:50:02 PM | Epoch[1] (19456/50000):\tLoss 0.1535\tAccurary 94.97%\t\tTime 16.59s\n",
      "01/21 06:50:02 PM | Epoch[1] (19456/50000):\tLoss 0.1535\tAccurary 94.97%\t\tTime 16.59s\n",
      "01/21 06:50:02 PM | Epoch[1] (19456/50000):\tLoss 0.1535\tAccurary 94.97%\t\tTime 16.59s\n",
      "01/21 06:50:18 PM | Epoch[1] (24320/50000):\tLoss 0.1548\tAccurary 94.87%\t\tTime 16.45s\n",
      "01/21 06:50:18 PM | Epoch[1] (24320/50000):\tLoss 0.1548\tAccurary 94.87%\t\tTime 16.45s\n",
      "01/21 06:50:18 PM | Epoch[1] (24320/50000):\tLoss 0.1548\tAccurary 94.87%\t\tTime 16.45s\n",
      "01/21 06:50:18 PM | Epoch[1] (24320/50000):\tLoss 0.1548\tAccurary 94.87%\t\tTime 16.45s\n",
      "01/21 06:50:18 PM | Epoch[1] (24320/50000):\tLoss 0.1548\tAccurary 94.87%\t\tTime 16.45s\n",
      "01/21 06:50:34 PM | Epoch[1] (29184/50000):\tLoss 0.1606\tAccurary 94.74%\t\tTime 15.96s\n",
      "01/21 06:50:34 PM | Epoch[1] (29184/50000):\tLoss 0.1606\tAccurary 94.74%\t\tTime 15.96s\n",
      "01/21 06:50:34 PM | Epoch[1] (29184/50000):\tLoss 0.1606\tAccurary 94.74%\t\tTime 15.96s\n",
      "01/21 06:50:34 PM | Epoch[1] (29184/50000):\tLoss 0.1606\tAccurary 94.74%\t\tTime 15.96s\n",
      "01/21 06:50:34 PM | Epoch[1] (29184/50000):\tLoss 0.1606\tAccurary 94.74%\t\tTime 15.96s\n",
      "01/21 06:50:50 PM | Epoch[1] (34048/50000):\tLoss 0.1612\tAccurary 94.69%\t\tTime 16.02s\n",
      "01/21 06:50:50 PM | Epoch[1] (34048/50000):\tLoss 0.1612\tAccurary 94.69%\t\tTime 16.02s\n",
      "01/21 06:50:50 PM | Epoch[1] (34048/50000):\tLoss 0.1612\tAccurary 94.69%\t\tTime 16.02s\n",
      "01/21 06:50:50 PM | Epoch[1] (34048/50000):\tLoss 0.1612\tAccurary 94.69%\t\tTime 16.02s\n",
      "01/21 06:50:50 PM | Epoch[1] (34048/50000):\tLoss 0.1612\tAccurary 94.69%\t\tTime 16.02s\n",
      "01/21 06:51:06 PM | Epoch[1] (38912/50000):\tLoss 0.1611\tAccurary 94.68%\t\tTime 15.99s\n",
      "01/21 06:51:06 PM | Epoch[1] (38912/50000):\tLoss 0.1611\tAccurary 94.68%\t\tTime 15.99s\n",
      "01/21 06:51:06 PM | Epoch[1] (38912/50000):\tLoss 0.1611\tAccurary 94.68%\t\tTime 15.99s\n",
      "01/21 06:51:06 PM | Epoch[1] (38912/50000):\tLoss 0.1611\tAccurary 94.68%\t\tTime 15.99s\n",
      "01/21 06:51:06 PM | Epoch[1] (38912/50000):\tLoss 0.1611\tAccurary 94.68%\t\tTime 15.99s\n",
      "01/21 06:51:22 PM | Epoch[1] (43776/50000):\tLoss 0.1617\tAccurary 94.65%\t\tTime 16.20s\n",
      "01/21 06:51:22 PM | Epoch[1] (43776/50000):\tLoss 0.1617\tAccurary 94.65%\t\tTime 16.20s\n",
      "01/21 06:51:22 PM | Epoch[1] (43776/50000):\tLoss 0.1617\tAccurary 94.65%\t\tTime 16.20s\n",
      "01/21 06:51:22 PM | Epoch[1] (43776/50000):\tLoss 0.1617\tAccurary 94.65%\t\tTime 16.20s\n",
      "01/21 06:51:22 PM | Epoch[1] (43776/50000):\tLoss 0.1617\tAccurary 94.65%\t\tTime 16.20s\n",
      "01/21 06:51:38 PM | Epoch[1] (48640/50000):\tLoss 0.1640\tAccurary 94.56%\t\tTime 16.02s\n",
      "01/21 06:51:38 PM | Epoch[1] (48640/50000):\tLoss 0.1640\tAccurary 94.56%\t\tTime 16.02s\n",
      "01/21 06:51:38 PM | Epoch[1] (48640/50000):\tLoss 0.1640\tAccurary 94.56%\t\tTime 16.02s\n",
      "01/21 06:51:38 PM | Epoch[1] (48640/50000):\tLoss 0.1640\tAccurary 94.56%\t\tTime 16.02s\n",
      "01/21 06:51:38 PM | Epoch[1] (48640/50000):\tLoss 0.1640\tAccurary 94.56%\t\tTime 16.02s\n",
      "01/21 06:51:47 PM | Test Loss 0.3711\tAccurary 87.96%\t\tTime 4.53s\n",
      "\n",
      "01/21 06:51:47 PM | Test Loss 0.3711\tAccurary 87.96%\t\tTime 4.53s\n",
      "\n",
      "01/21 06:51:47 PM | Test Loss 0.3711\tAccurary 87.96%\t\tTime 4.53s\n",
      "\n",
      "01/21 06:51:47 PM | Test Loss 0.3711\tAccurary 87.96%\t\tTime 4.53s\n",
      "\n",
      "01/21 06:51:47 PM | Test Loss 0.3711\tAccurary 87.96%\t\tTime 4.53s\n",
      "\n",
      "01/21 06:51:47 PM | ############# model1_2epoch:87.95999908447266 using 352.1791751384735s #################\n",
      "01/21 06:51:47 PM | ############# model1_2epoch:87.95999908447266 using 352.1791751384735s #################\n",
      "01/21 06:51:47 PM | ############# model1_2epoch:87.95999908447266 using 352.1791751384735s #################\n",
      "01/21 06:51:47 PM | ############# model1_2epoch:87.95999908447266 using 352.1791751384735s #################\n",
      "01/21 06:51:47 PM | ############# model1_2epoch:87.95999908447266 using 352.1791751384735s #################\n",
      "01/21 06:52:09 PM | Epoch[0] (4864/50000):\tLoss 0.4864\tAccurary 84.94%\t\tTime 22.47s\n",
      "01/21 06:52:09 PM | Epoch[0] (4864/50000):\tLoss 0.4864\tAccurary 84.94%\t\tTime 22.47s\n",
      "01/21 06:52:09 PM | Epoch[0] (4864/50000):\tLoss 0.4864\tAccurary 84.94%\t\tTime 22.47s\n",
      "01/21 06:52:09 PM | Epoch[0] (4864/50000):\tLoss 0.4864\tAccurary 84.94%\t\tTime 22.47s\n",
      "01/21 06:52:09 PM | Epoch[0] (4864/50000):\tLoss 0.4864\tAccurary 84.94%\t\tTime 22.47s\n",
      "01/21 06:52:27 PM | Epoch[0] (9728/50000):\tLoss 0.3771\tAccurary 87.78%\t\tTime 17.83s\n",
      "01/21 06:52:27 PM | Epoch[0] (9728/50000):\tLoss 0.3771\tAccurary 87.78%\t\tTime 17.83s\n",
      "01/21 06:52:27 PM | Epoch[0] (9728/50000):\tLoss 0.3771\tAccurary 87.78%\t\tTime 17.83s\n",
      "01/21 06:52:27 PM | Epoch[0] (9728/50000):\tLoss 0.3771\tAccurary 87.78%\t\tTime 17.83s\n",
      "01/21 06:52:27 PM | Epoch[0] (9728/50000):\tLoss 0.3771\tAccurary 87.78%\t\tTime 17.83s\n",
      "01/21 06:52:45 PM | Epoch[0] (14592/50000):\tLoss 0.3245\tAccurary 89.45%\t\tTime 17.69s\n",
      "01/21 06:52:45 PM | Epoch[0] (14592/50000):\tLoss 0.3245\tAccurary 89.45%\t\tTime 17.69s\n",
      "01/21 06:52:45 PM | Epoch[0] (14592/50000):\tLoss 0.3245\tAccurary 89.45%\t\tTime 17.69s\n",
      "01/21 06:52:45 PM | Epoch[0] (14592/50000):\tLoss 0.3245\tAccurary 89.45%\t\tTime 17.69s\n",
      "01/21 06:52:45 PM | Epoch[0] (14592/50000):\tLoss 0.3245\tAccurary 89.45%\t\tTime 17.69s\n",
      "01/21 06:53:03 PM | Epoch[0] (19456/50000):\tLoss 0.2895\tAccurary 90.59%\t\tTime 17.72s\n",
      "01/21 06:53:03 PM | Epoch[0] (19456/50000):\tLoss 0.2895\tAccurary 90.59%\t\tTime 17.72s\n",
      "01/21 06:53:03 PM | Epoch[0] (19456/50000):\tLoss 0.2895\tAccurary 90.59%\t\tTime 17.72s\n",
      "01/21 06:53:03 PM | Epoch[0] (19456/50000):\tLoss 0.2895\tAccurary 90.59%\t\tTime 17.72s\n",
      "01/21 06:53:03 PM | Epoch[0] (19456/50000):\tLoss 0.2895\tAccurary 90.59%\t\tTime 17.72s\n",
      "01/21 06:53:20 PM | Epoch[0] (24320/50000):\tLoss 0.2653\tAccurary 91.34%\t\tTime 17.77s\n",
      "01/21 06:53:20 PM | Epoch[0] (24320/50000):\tLoss 0.2653\tAccurary 91.34%\t\tTime 17.77s\n",
      "01/21 06:53:20 PM | Epoch[0] (24320/50000):\tLoss 0.2653\tAccurary 91.34%\t\tTime 17.77s\n",
      "01/21 06:53:20 PM | Epoch[0] (24320/50000):\tLoss 0.2653\tAccurary 91.34%\t\tTime 17.77s\n",
      "01/21 06:53:20 PM | Epoch[0] (24320/50000):\tLoss 0.2653\tAccurary 91.34%\t\tTime 17.77s\n",
      "01/21 06:53:38 PM | Epoch[0] (29184/50000):\tLoss 0.2518\tAccurary 91.72%\t\tTime 17.72s\n",
      "01/21 06:53:38 PM | Epoch[0] (29184/50000):\tLoss 0.2518\tAccurary 91.72%\t\tTime 17.72s\n",
      "01/21 06:53:38 PM | Epoch[0] (29184/50000):\tLoss 0.2518\tAccurary 91.72%\t\tTime 17.72s\n",
      "01/21 06:53:38 PM | Epoch[0] (29184/50000):\tLoss 0.2518\tAccurary 91.72%\t\tTime 17.72s\n",
      "01/21 06:53:38 PM | Epoch[0] (29184/50000):\tLoss 0.2518\tAccurary 91.72%\t\tTime 17.72s\n",
      "01/21 06:53:56 PM | Epoch[0] (34048/50000):\tLoss 0.2416\tAccurary 92.04%\t\tTime 17.91s\n",
      "01/21 06:53:56 PM | Epoch[0] (34048/50000):\tLoss 0.2416\tAccurary 92.04%\t\tTime 17.91s\n",
      "01/21 06:53:56 PM | Epoch[0] (34048/50000):\tLoss 0.2416\tAccurary 92.04%\t\tTime 17.91s\n",
      "01/21 06:53:56 PM | Epoch[0] (34048/50000):\tLoss 0.2416\tAccurary 92.04%\t\tTime 17.91s\n",
      "01/21 06:53:56 PM | Epoch[0] (34048/50000):\tLoss 0.2416\tAccurary 92.04%\t\tTime 17.91s\n",
      "01/21 06:54:14 PM | Epoch[0] (38912/50000):\tLoss 0.2326\tAccurary 92.30%\t\tTime 17.93s\n",
      "01/21 06:54:14 PM | Epoch[0] (38912/50000):\tLoss 0.2326\tAccurary 92.30%\t\tTime 17.93s\n",
      "01/21 06:54:14 PM | Epoch[0] (38912/50000):\tLoss 0.2326\tAccurary 92.30%\t\tTime 17.93s\n",
      "01/21 06:54:14 PM | Epoch[0] (38912/50000):\tLoss 0.2326\tAccurary 92.30%\t\tTime 17.93s\n",
      "01/21 06:54:14 PM | Epoch[0] (38912/50000):\tLoss 0.2326\tAccurary 92.30%\t\tTime 17.93s\n",
      "01/21 06:54:32 PM | Epoch[0] (43776/50000):\tLoss 0.2251\tAccurary 92.53%\t\tTime 17.81s\n",
      "01/21 06:54:32 PM | Epoch[0] (43776/50000):\tLoss 0.2251\tAccurary 92.53%\t\tTime 17.81s\n",
      "01/21 06:54:32 PM | Epoch[0] (43776/50000):\tLoss 0.2251\tAccurary 92.53%\t\tTime 17.81s\n",
      "01/21 06:54:32 PM | Epoch[0] (43776/50000):\tLoss 0.2251\tAccurary 92.53%\t\tTime 17.81s\n",
      "01/21 06:54:32 PM | Epoch[0] (43776/50000):\tLoss 0.2251\tAccurary 92.53%\t\tTime 17.81s\n",
      "01/21 06:54:49 PM | Epoch[0] (48640/50000):\tLoss 0.2201\tAccurary 92.72%\t\tTime 17.66s\n",
      "01/21 06:54:49 PM | Epoch[0] (48640/50000):\tLoss 0.2201\tAccurary 92.72%\t\tTime 17.66s\n",
      "01/21 06:54:49 PM | Epoch[0] (48640/50000):\tLoss 0.2201\tAccurary 92.72%\t\tTime 17.66s\n",
      "01/21 06:54:49 PM | Epoch[0] (48640/50000):\tLoss 0.2201\tAccurary 92.72%\t\tTime 17.66s\n",
      "01/21 06:54:49 PM | Epoch[0] (48640/50000):\tLoss 0.2201\tAccurary 92.72%\t\tTime 17.66s\n",
      "01/21 06:54:59 PM | Test Loss 0.3663\tAccurary 88.31%\t\tTime 5.25s\n",
      "\n",
      "01/21 06:54:59 PM | Test Loss 0.3663\tAccurary 88.31%\t\tTime 5.25s\n",
      "\n",
      "01/21 06:54:59 PM | Test Loss 0.3663\tAccurary 88.31%\t\tTime 5.25s\n",
      "\n",
      "01/21 06:54:59 PM | Test Loss 0.3663\tAccurary 88.31%\t\tTime 5.25s\n",
      "\n",
      "01/21 06:54:59 PM | Test Loss 0.3663\tAccurary 88.31%\t\tTime 5.25s\n",
      "\n",
      "01/21 06:55:21 PM | Epoch[1] (4864/50000):\tLoss 0.1671\tAccurary 94.39%\t\tTime 22.52s\n",
      "01/21 06:55:21 PM | Epoch[1] (4864/50000):\tLoss 0.1671\tAccurary 94.39%\t\tTime 22.52s\n",
      "01/21 06:55:21 PM | Epoch[1] (4864/50000):\tLoss 0.1671\tAccurary 94.39%\t\tTime 22.52s\n",
      "01/21 06:55:21 PM | Epoch[1] (4864/50000):\tLoss 0.1671\tAccurary 94.39%\t\tTime 22.52s\n",
      "01/21 06:55:21 PM | Epoch[1] (4864/50000):\tLoss 0.1671\tAccurary 94.39%\t\tTime 22.52s\n",
      "01/21 06:55:39 PM | Epoch[1] (9728/50000):\tLoss 0.1584\tAccurary 94.68%\t\tTime 17.70s\n",
      "01/21 06:55:39 PM | Epoch[1] (9728/50000):\tLoss 0.1584\tAccurary 94.68%\t\tTime 17.70s\n",
      "01/21 06:55:39 PM | Epoch[1] (9728/50000):\tLoss 0.1584\tAccurary 94.68%\t\tTime 17.70s\n",
      "01/21 06:55:39 PM | Epoch[1] (9728/50000):\tLoss 0.1584\tAccurary 94.68%\t\tTime 17.70s\n",
      "01/21 06:55:39 PM | Epoch[1] (9728/50000):\tLoss 0.1584\tAccurary 94.68%\t\tTime 17.70s\n",
      "01/21 06:55:57 PM | Epoch[1] (14592/50000):\tLoss 0.1575\tAccurary 94.73%\t\tTime 18.28s\n",
      "01/21 06:55:57 PM | Epoch[1] (14592/50000):\tLoss 0.1575\tAccurary 94.73%\t\tTime 18.28s\n",
      "01/21 06:55:57 PM | Epoch[1] (14592/50000):\tLoss 0.1575\tAccurary 94.73%\t\tTime 18.28s\n",
      "01/21 06:55:57 PM | Epoch[1] (14592/50000):\tLoss 0.1575\tAccurary 94.73%\t\tTime 18.28s\n",
      "01/21 06:55:57 PM | Epoch[1] (14592/50000):\tLoss 0.1575\tAccurary 94.73%\t\tTime 18.28s\n",
      "01/21 06:56:15 PM | Epoch[1] (19456/50000):\tLoss 0.1628\tAccurary 94.46%\t\tTime 17.87s\n",
      "01/21 06:56:15 PM | Epoch[1] (19456/50000):\tLoss 0.1628\tAccurary 94.46%\t\tTime 17.87s\n",
      "01/21 06:56:15 PM | Epoch[1] (19456/50000):\tLoss 0.1628\tAccurary 94.46%\t\tTime 17.87s\n",
      "01/21 06:56:15 PM | Epoch[1] (19456/50000):\tLoss 0.1628\tAccurary 94.46%\t\tTime 17.87s\n",
      "01/21 06:56:15 PM | Epoch[1] (19456/50000):\tLoss 0.1628\tAccurary 94.46%\t\tTime 17.87s\n",
      "01/21 06:56:34 PM | Epoch[1] (24320/50000):\tLoss 0.1635\tAccurary 94.45%\t\tTime 18.78s\n",
      "01/21 06:56:34 PM | Epoch[1] (24320/50000):\tLoss 0.1635\tAccurary 94.45%\t\tTime 18.78s\n",
      "01/21 06:56:34 PM | Epoch[1] (24320/50000):\tLoss 0.1635\tAccurary 94.45%\t\tTime 18.78s\n",
      "01/21 06:56:34 PM | Epoch[1] (24320/50000):\tLoss 0.1635\tAccurary 94.45%\t\tTime 18.78s\n",
      "01/21 06:56:34 PM | Epoch[1] (24320/50000):\tLoss 0.1635\tAccurary 94.45%\t\tTime 18.78s\n",
      "01/21 06:56:53 PM | Epoch[1] (29184/50000):\tLoss 0.1656\tAccurary 94.40%\t\tTime 18.68s\n",
      "01/21 06:56:53 PM | Epoch[1] (29184/50000):\tLoss 0.1656\tAccurary 94.40%\t\tTime 18.68s\n",
      "01/21 06:56:53 PM | Epoch[1] (29184/50000):\tLoss 0.1656\tAccurary 94.40%\t\tTime 18.68s\n",
      "01/21 06:56:53 PM | Epoch[1] (29184/50000):\tLoss 0.1656\tAccurary 94.40%\t\tTime 18.68s\n",
      "01/21 06:56:53 PM | Epoch[1] (29184/50000):\tLoss 0.1656\tAccurary 94.40%\t\tTime 18.68s\n",
      "01/21 06:57:12 PM | Epoch[1] (34048/50000):\tLoss 0.1663\tAccurary 94.35%\t\tTime 19.20s\n",
      "01/21 06:57:12 PM | Epoch[1] (34048/50000):\tLoss 0.1663\tAccurary 94.35%\t\tTime 19.20s\n",
      "01/21 06:57:12 PM | Epoch[1] (34048/50000):\tLoss 0.1663\tAccurary 94.35%\t\tTime 19.20s\n",
      "01/21 06:57:12 PM | Epoch[1] (34048/50000):\tLoss 0.1663\tAccurary 94.35%\t\tTime 19.20s\n",
      "01/21 06:57:12 PM | Epoch[1] (34048/50000):\tLoss 0.1663\tAccurary 94.35%\t\tTime 19.20s\n",
      "01/21 06:57:31 PM | Epoch[1] (38912/50000):\tLoss 0.1660\tAccurary 94.37%\t\tTime 18.71s\n",
      "01/21 06:57:31 PM | Epoch[1] (38912/50000):\tLoss 0.1660\tAccurary 94.37%\t\tTime 18.71s\n",
      "01/21 06:57:31 PM | Epoch[1] (38912/50000):\tLoss 0.1660\tAccurary 94.37%\t\tTime 18.71s\n",
      "01/21 06:57:31 PM | Epoch[1] (38912/50000):\tLoss 0.1660\tAccurary 94.37%\t\tTime 18.71s\n",
      "01/21 06:57:31 PM | Epoch[1] (38912/50000):\tLoss 0.1660\tAccurary 94.37%\t\tTime 18.71s\n",
      "01/21 06:57:49 PM | Epoch[1] (43776/50000):\tLoss 0.1678\tAccurary 94.29%\t\tTime 18.62s\n",
      "01/21 06:57:49 PM | Epoch[1] (43776/50000):\tLoss 0.1678\tAccurary 94.29%\t\tTime 18.62s\n",
      "01/21 06:57:49 PM | Epoch[1] (43776/50000):\tLoss 0.1678\tAccurary 94.29%\t\tTime 18.62s\n",
      "01/21 06:57:49 PM | Epoch[1] (43776/50000):\tLoss 0.1678\tAccurary 94.29%\t\tTime 18.62s\n",
      "01/21 06:57:49 PM | Epoch[1] (43776/50000):\tLoss 0.1678\tAccurary 94.29%\t\tTime 18.62s\n",
      "01/21 06:58:07 PM | Epoch[1] (48640/50000):\tLoss 0.1695\tAccurary 94.23%\t\tTime 17.82s\n",
      "01/21 06:58:07 PM | Epoch[1] (48640/50000):\tLoss 0.1695\tAccurary 94.23%\t\tTime 17.82s\n",
      "01/21 06:58:07 PM | Epoch[1] (48640/50000):\tLoss 0.1695\tAccurary 94.23%\t\tTime 17.82s\n",
      "01/21 06:58:07 PM | Epoch[1] (48640/50000):\tLoss 0.1695\tAccurary 94.23%\t\tTime 17.82s\n",
      "01/21 06:58:07 PM | Epoch[1] (48640/50000):\tLoss 0.1695\tAccurary 94.23%\t\tTime 17.82s\n",
      "01/21 06:58:17 PM | Test Loss 0.3918\tAccurary 87.20%\t\tTime 5.50s\n",
      "\n",
      "01/21 06:58:17 PM | Test Loss 0.3918\tAccurary 87.20%\t\tTime 5.50s\n",
      "\n",
      "01/21 06:58:17 PM | Test Loss 0.3918\tAccurary 87.20%\t\tTime 5.50s\n",
      "\n",
      "01/21 06:58:17 PM | Test Loss 0.3918\tAccurary 87.20%\t\tTime 5.50s\n",
      "\n",
      "01/21 06:58:17 PM | Test Loss 0.3918\tAccurary 87.20%\t\tTime 5.50s\n",
      "\n",
      "01/21 06:58:17 PM | ############# model2_2epoch:88.30999755859375 using 389.6251928806305s #################\n",
      "01/21 06:58:17 PM | ############# model2_2epoch:88.30999755859375 using 389.6251928806305s #################\n",
      "01/21 06:58:17 PM | ############# model2_2epoch:88.30999755859375 using 389.6251928806305s #################\n",
      "01/21 06:58:17 PM | ############# model2_2epoch:88.30999755859375 using 389.6251928806305s #################\n",
      "01/21 06:58:17 PM | ############# model2_2epoch:88.30999755859375 using 389.6251928806305s #################\n",
      "01/21 06:58:33 PM | Epoch[0] (4864/50000):\tLoss 0.5288\tAccurary 84.04%\t\tTime 16.28s\n",
      "01/21 06:58:33 PM | Epoch[0] (4864/50000):\tLoss 0.5288\tAccurary 84.04%\t\tTime 16.28s\n",
      "01/21 06:58:33 PM | Epoch[0] (4864/50000):\tLoss 0.5288\tAccurary 84.04%\t\tTime 16.28s\n",
      "01/21 06:58:33 PM | Epoch[0] (4864/50000):\tLoss 0.5288\tAccurary 84.04%\t\tTime 16.28s\n",
      "01/21 06:58:33 PM | Epoch[0] (4864/50000):\tLoss 0.5288\tAccurary 84.04%\t\tTime 16.28s\n",
      "01/21 06:58:46 PM | Epoch[0] (9728/50000):\tLoss 0.4208\tAccurary 86.99%\t\tTime 12.79s\n",
      "01/21 06:58:46 PM | Epoch[0] (9728/50000):\tLoss 0.4208\tAccurary 86.99%\t\tTime 12.79s\n",
      "01/21 06:58:46 PM | Epoch[0] (9728/50000):\tLoss 0.4208\tAccurary 86.99%\t\tTime 12.79s\n",
      "01/21 06:58:46 PM | Epoch[0] (9728/50000):\tLoss 0.4208\tAccurary 86.99%\t\tTime 12.79s\n",
      "01/21 06:58:46 PM | Epoch[0] (9728/50000):\tLoss 0.4208\tAccurary 86.99%\t\tTime 12.79s\n",
      "01/21 06:58:58 PM | Epoch[0] (14592/50000):\tLoss 0.3617\tAccurary 88.58%\t\tTime 12.27s\n",
      "01/21 06:58:58 PM | Epoch[0] (14592/50000):\tLoss 0.3617\tAccurary 88.58%\t\tTime 12.27s\n",
      "01/21 06:58:58 PM | Epoch[0] (14592/50000):\tLoss 0.3617\tAccurary 88.58%\t\tTime 12.27s\n",
      "01/21 06:58:58 PM | Epoch[0] (14592/50000):\tLoss 0.3617\tAccurary 88.58%\t\tTime 12.27s\n",
      "01/21 06:58:58 PM | Epoch[0] (14592/50000):\tLoss 0.3617\tAccurary 88.58%\t\tTime 12.27s\n",
      "01/21 06:59:10 PM | Epoch[0] (19456/50000):\tLoss 0.3229\tAccurary 89.68%\t\tTime 12.04s\n",
      "01/21 06:59:10 PM | Epoch[0] (19456/50000):\tLoss 0.3229\tAccurary 89.68%\t\tTime 12.04s\n",
      "01/21 06:59:10 PM | Epoch[0] (19456/50000):\tLoss 0.3229\tAccurary 89.68%\t\tTime 12.04s\n",
      "01/21 06:59:10 PM | Epoch[0] (19456/50000):\tLoss 0.3229\tAccurary 89.68%\t\tTime 12.04s\n",
      "01/21 06:59:10 PM | Epoch[0] (19456/50000):\tLoss 0.3229\tAccurary 89.68%\t\tTime 12.04s\n",
      "01/21 06:59:22 PM | Epoch[0] (24320/50000):\tLoss 0.2984\tAccurary 90.34%\t\tTime 11.93s\n",
      "01/21 06:59:22 PM | Epoch[0] (24320/50000):\tLoss 0.2984\tAccurary 90.34%\t\tTime 11.93s\n",
      "01/21 06:59:22 PM | Epoch[0] (24320/50000):\tLoss 0.2984\tAccurary 90.34%\t\tTime 11.93s\n",
      "01/21 06:59:22 PM | Epoch[0] (24320/50000):\tLoss 0.2984\tAccurary 90.34%\t\tTime 11.93s\n",
      "01/21 06:59:22 PM | Epoch[0] (24320/50000):\tLoss 0.2984\tAccurary 90.34%\t\tTime 11.93s\n",
      "01/21 06:59:34 PM | Epoch[0] (29184/50000):\tLoss 0.2836\tAccurary 90.71%\t\tTime 11.98s\n",
      "01/21 06:59:34 PM | Epoch[0] (29184/50000):\tLoss 0.2836\tAccurary 90.71%\t\tTime 11.98s\n",
      "01/21 06:59:34 PM | Epoch[0] (29184/50000):\tLoss 0.2836\tAccurary 90.71%\t\tTime 11.98s\n",
      "01/21 06:59:34 PM | Epoch[0] (29184/50000):\tLoss 0.2836\tAccurary 90.71%\t\tTime 11.98s\n",
      "01/21 06:59:34 PM | Epoch[0] (29184/50000):\tLoss 0.2836\tAccurary 90.71%\t\tTime 11.98s\n",
      "01/21 06:59:46 PM | Epoch[0] (34048/50000):\tLoss 0.2715\tAccurary 91.05%\t\tTime 12.19s\n",
      "01/21 06:59:46 PM | Epoch[0] (34048/50000):\tLoss 0.2715\tAccurary 91.05%\t\tTime 12.19s\n",
      "01/21 06:59:46 PM | Epoch[0] (34048/50000):\tLoss 0.2715\tAccurary 91.05%\t\tTime 12.19s\n",
      "01/21 06:59:46 PM | Epoch[0] (34048/50000):\tLoss 0.2715\tAccurary 91.05%\t\tTime 12.19s\n",
      "01/21 06:59:46 PM | Epoch[0] (34048/50000):\tLoss 0.2715\tAccurary 91.05%\t\tTime 12.19s\n",
      "01/21 06:59:58 PM | Epoch[0] (38912/50000):\tLoss 0.2603\tAccurary 91.37%\t\tTime 11.60s\n",
      "01/21 06:59:58 PM | Epoch[0] (38912/50000):\tLoss 0.2603\tAccurary 91.37%\t\tTime 11.60s\n",
      "01/21 06:59:58 PM | Epoch[0] (38912/50000):\tLoss 0.2603\tAccurary 91.37%\t\tTime 11.60s\n",
      "01/21 06:59:58 PM | Epoch[0] (38912/50000):\tLoss 0.2603\tAccurary 91.37%\t\tTime 11.60s\n",
      "01/21 06:59:58 PM | Epoch[0] (38912/50000):\tLoss 0.2603\tAccurary 91.37%\t\tTime 11.60s\n",
      "01/21 07:00:10 PM | Epoch[0] (43776/50000):\tLoss 0.2525\tAccurary 91.59%\t\tTime 11.60s\n",
      "01/21 07:00:10 PM | Epoch[0] (43776/50000):\tLoss 0.2525\tAccurary 91.59%\t\tTime 11.60s\n",
      "01/21 07:00:10 PM | Epoch[0] (43776/50000):\tLoss 0.2525\tAccurary 91.59%\t\tTime 11.60s\n",
      "01/21 07:00:10 PM | Epoch[0] (43776/50000):\tLoss 0.2525\tAccurary 91.59%\t\tTime 11.60s\n",
      "01/21 07:00:10 PM | Epoch[0] (43776/50000):\tLoss 0.2525\tAccurary 91.59%\t\tTime 11.60s\n",
      "01/21 07:00:21 PM | Epoch[0] (48640/50000):\tLoss 0.2461\tAccurary 91.82%\t\tTime 11.61s\n",
      "01/21 07:00:21 PM | Epoch[0] (48640/50000):\tLoss 0.2461\tAccurary 91.82%\t\tTime 11.61s\n",
      "01/21 07:00:21 PM | Epoch[0] (48640/50000):\tLoss 0.2461\tAccurary 91.82%\t\tTime 11.61s\n",
      "01/21 07:00:21 PM | Epoch[0] (48640/50000):\tLoss 0.2461\tAccurary 91.82%\t\tTime 11.61s\n",
      "01/21 07:00:21 PM | Epoch[0] (48640/50000):\tLoss 0.2461\tAccurary 91.82%\t\tTime 11.61s\n",
      "01/21 07:00:28 PM | Test Loss 0.3492\tAccurary 88.82%\t\tTime 4.35s\n",
      "\n",
      "01/21 07:00:28 PM | Test Loss 0.3492\tAccurary 88.82%\t\tTime 4.35s\n",
      "\n",
      "01/21 07:00:28 PM | Test Loss 0.3492\tAccurary 88.82%\t\tTime 4.35s\n",
      "\n",
      "01/21 07:00:28 PM | Test Loss 0.3492\tAccurary 88.82%\t\tTime 4.35s\n",
      "\n",
      "01/21 07:00:28 PM | Test Loss 0.3492\tAccurary 88.82%\t\tTime 4.35s\n",
      "\n",
      "01/21 07:00:45 PM | Epoch[1] (4864/50000):\tLoss 0.1579\tAccurary 94.69%\t\tTime 16.18s\n",
      "01/21 07:00:45 PM | Epoch[1] (4864/50000):\tLoss 0.1579\tAccurary 94.69%\t\tTime 16.18s\n",
      "01/21 07:00:45 PM | Epoch[1] (4864/50000):\tLoss 0.1579\tAccurary 94.69%\t\tTime 16.18s\n",
      "01/21 07:00:45 PM | Epoch[1] (4864/50000):\tLoss 0.1579\tAccurary 94.69%\t\tTime 16.18s\n",
      "01/21 07:00:45 PM | Epoch[1] (4864/50000):\tLoss 0.1579\tAccurary 94.69%\t\tTime 16.18s\n",
      "01/21 07:00:56 PM | Epoch[1] (9728/50000):\tLoss 0.1555\tAccurary 94.73%\t\tTime 11.61s\n",
      "01/21 07:00:56 PM | Epoch[1] (9728/50000):\tLoss 0.1555\tAccurary 94.73%\t\tTime 11.61s\n",
      "01/21 07:00:56 PM | Epoch[1] (9728/50000):\tLoss 0.1555\tAccurary 94.73%\t\tTime 11.61s\n",
      "01/21 07:00:56 PM | Epoch[1] (9728/50000):\tLoss 0.1555\tAccurary 94.73%\t\tTime 11.61s\n",
      "01/21 07:00:56 PM | Epoch[1] (9728/50000):\tLoss 0.1555\tAccurary 94.73%\t\tTime 11.61s\n",
      "01/21 07:01:08 PM | Epoch[1] (14592/50000):\tLoss 0.1570\tAccurary 94.77%\t\tTime 11.60s\n",
      "01/21 07:01:08 PM | Epoch[1] (14592/50000):\tLoss 0.1570\tAccurary 94.77%\t\tTime 11.60s\n",
      "01/21 07:01:08 PM | Epoch[1] (14592/50000):\tLoss 0.1570\tAccurary 94.77%\t\tTime 11.60s\n",
      "01/21 07:01:08 PM | Epoch[1] (14592/50000):\tLoss 0.1570\tAccurary 94.77%\t\tTime 11.60s\n",
      "01/21 07:01:08 PM | Epoch[1] (14592/50000):\tLoss 0.1570\tAccurary 94.77%\t\tTime 11.60s\n",
      "01/21 07:01:19 PM | Epoch[1] (19456/50000):\tLoss 0.1577\tAccurary 94.78%\t\tTime 11.60s\n",
      "01/21 07:01:19 PM | Epoch[1] (19456/50000):\tLoss 0.1577\tAccurary 94.78%\t\tTime 11.60s\n",
      "01/21 07:01:19 PM | Epoch[1] (19456/50000):\tLoss 0.1577\tAccurary 94.78%\t\tTime 11.60s\n",
      "01/21 07:01:19 PM | Epoch[1] (19456/50000):\tLoss 0.1577\tAccurary 94.78%\t\tTime 11.60s\n",
      "01/21 07:01:19 PM | Epoch[1] (19456/50000):\tLoss 0.1577\tAccurary 94.78%\t\tTime 11.60s\n",
      "01/21 07:01:31 PM | Epoch[1] (24320/50000):\tLoss 0.1564\tAccurary 94.81%\t\tTime 11.60s\n",
      "01/21 07:01:31 PM | Epoch[1] (24320/50000):\tLoss 0.1564\tAccurary 94.81%\t\tTime 11.60s\n",
      "01/21 07:01:31 PM | Epoch[1] (24320/50000):\tLoss 0.1564\tAccurary 94.81%\t\tTime 11.60s\n",
      "01/21 07:01:31 PM | Epoch[1] (24320/50000):\tLoss 0.1564\tAccurary 94.81%\t\tTime 11.60s\n",
      "01/21 07:01:31 PM | Epoch[1] (24320/50000):\tLoss 0.1564\tAccurary 94.81%\t\tTime 11.60s\n",
      "01/21 07:01:43 PM | Epoch[1] (29184/50000):\tLoss 0.1584\tAccurary 94.72%\t\tTime 11.61s\n",
      "01/21 07:01:43 PM | Epoch[1] (29184/50000):\tLoss 0.1584\tAccurary 94.72%\t\tTime 11.61s\n",
      "01/21 07:01:43 PM | Epoch[1] (29184/50000):\tLoss 0.1584\tAccurary 94.72%\t\tTime 11.61s\n",
      "01/21 07:01:43 PM | Epoch[1] (29184/50000):\tLoss 0.1584\tAccurary 94.72%\t\tTime 11.61s\n",
      "01/21 07:01:43 PM | Epoch[1] (29184/50000):\tLoss 0.1584\tAccurary 94.72%\t\tTime 11.61s\n",
      "01/21 07:01:54 PM | Epoch[1] (34048/50000):\tLoss 0.1627\tAccurary 94.61%\t\tTime 11.53s\n",
      "01/21 07:01:54 PM | Epoch[1] (34048/50000):\tLoss 0.1627\tAccurary 94.61%\t\tTime 11.53s\n",
      "01/21 07:01:54 PM | Epoch[1] (34048/50000):\tLoss 0.1627\tAccurary 94.61%\t\tTime 11.53s\n",
      "01/21 07:01:54 PM | Epoch[1] (34048/50000):\tLoss 0.1627\tAccurary 94.61%\t\tTime 11.53s\n",
      "01/21 07:01:54 PM | Epoch[1] (34048/50000):\tLoss 0.1627\tAccurary 94.61%\t\tTime 11.53s\n",
      "01/21 07:02:06 PM | Epoch[1] (38912/50000):\tLoss 0.1634\tAccurary 94.58%\t\tTime 11.57s\n",
      "01/21 07:02:06 PM | Epoch[1] (38912/50000):\tLoss 0.1634\tAccurary 94.58%\t\tTime 11.57s\n",
      "01/21 07:02:06 PM | Epoch[1] (38912/50000):\tLoss 0.1634\tAccurary 94.58%\t\tTime 11.57s\n",
      "01/21 07:02:06 PM | Epoch[1] (38912/50000):\tLoss 0.1634\tAccurary 94.58%\t\tTime 11.57s\n",
      "01/21 07:02:06 PM | Epoch[1] (38912/50000):\tLoss 0.1634\tAccurary 94.58%\t\tTime 11.57s\n",
      "01/21 07:02:17 PM | Epoch[1] (43776/50000):\tLoss 0.1651\tAccurary 94.52%\t\tTime 11.63s\n",
      "01/21 07:02:17 PM | Epoch[1] (43776/50000):\tLoss 0.1651\tAccurary 94.52%\t\tTime 11.63s\n",
      "01/21 07:02:17 PM | Epoch[1] (43776/50000):\tLoss 0.1651\tAccurary 94.52%\t\tTime 11.63s\n",
      "01/21 07:02:17 PM | Epoch[1] (43776/50000):\tLoss 0.1651\tAccurary 94.52%\t\tTime 11.63s\n",
      "01/21 07:02:17 PM | Epoch[1] (43776/50000):\tLoss 0.1651\tAccurary 94.52%\t\tTime 11.63s\n",
      "01/21 07:02:29 PM | Epoch[1] (48640/50000):\tLoss 0.1671\tAccurary 94.46%\t\tTime 11.59s\n",
      "01/21 07:02:29 PM | Epoch[1] (48640/50000):\tLoss 0.1671\tAccurary 94.46%\t\tTime 11.59s\n",
      "01/21 07:02:29 PM | Epoch[1] (48640/50000):\tLoss 0.1671\tAccurary 94.46%\t\tTime 11.59s\n",
      "01/21 07:02:29 PM | Epoch[1] (48640/50000):\tLoss 0.1671\tAccurary 94.46%\t\tTime 11.59s\n",
      "01/21 07:02:29 PM | Epoch[1] (48640/50000):\tLoss 0.1671\tAccurary 94.46%\t\tTime 11.59s\n",
      "01/21 07:02:36 PM | Test Loss 0.5099\tAccurary 84.06%\t\tTime 4.11s\n",
      "\n",
      "01/21 07:02:36 PM | Test Loss 0.5099\tAccurary 84.06%\t\tTime 4.11s\n",
      "\n",
      "01/21 07:02:36 PM | Test Loss 0.5099\tAccurary 84.06%\t\tTime 4.11s\n",
      "\n",
      "01/21 07:02:36 PM | Test Loss 0.5099\tAccurary 84.06%\t\tTime 4.11s\n",
      "\n",
      "01/21 07:02:36 PM | Test Loss 0.5099\tAccurary 84.06%\t\tTime 4.11s\n",
      "\n",
      "01/21 07:02:36 PM | ############# model3_2epoch:88.81999969482422 using 258.86367440223694s #################\n",
      "01/21 07:02:36 PM | ############# model3_2epoch:88.81999969482422 using 258.86367440223694s #################\n",
      "01/21 07:02:36 PM | ############# model3_2epoch:88.81999969482422 using 258.86367440223694s #################\n",
      "01/21 07:02:36 PM | ############# model3_2epoch:88.81999969482422 using 258.86367440223694s #################\n",
      "01/21 07:02:36 PM | ############# model3_2epoch:88.81999969482422 using 258.86367440223694s #################\n",
      "01/21 07:02:52 PM | Epoch[0] (4864/50000):\tLoss 0.3894\tAccurary 88.26%\t\tTime 16.40s\n",
      "01/21 07:02:52 PM | Epoch[0] (4864/50000):\tLoss 0.3894\tAccurary 88.26%\t\tTime 16.40s\n",
      "01/21 07:02:52 PM | Epoch[0] (4864/50000):\tLoss 0.3894\tAccurary 88.26%\t\tTime 16.40s\n",
      "01/21 07:02:52 PM | Epoch[0] (4864/50000):\tLoss 0.3894\tAccurary 88.26%\t\tTime 16.40s\n",
      "01/21 07:02:52 PM | Epoch[0] (4864/50000):\tLoss 0.3894\tAccurary 88.26%\t\tTime 16.40s\n",
      "01/21 07:03:04 PM | Epoch[0] (9728/50000):\tLoss 0.2992\tAccurary 90.84%\t\tTime 11.86s\n",
      "01/21 07:03:04 PM | Epoch[0] (9728/50000):\tLoss 0.2992\tAccurary 90.84%\t\tTime 11.86s\n",
      "01/21 07:03:04 PM | Epoch[0] (9728/50000):\tLoss 0.2992\tAccurary 90.84%\t\tTime 11.86s\n",
      "01/21 07:03:04 PM | Epoch[0] (9728/50000):\tLoss 0.2992\tAccurary 90.84%\t\tTime 11.86s\n",
      "01/21 07:03:04 PM | Epoch[0] (9728/50000):\tLoss 0.2992\tAccurary 90.84%\t\tTime 11.86s\n",
      "01/21 07:03:16 PM | Epoch[0] (14592/50000):\tLoss 0.2607\tAccurary 91.90%\t\tTime 11.90s\n",
      "01/21 07:03:16 PM | Epoch[0] (14592/50000):\tLoss 0.2607\tAccurary 91.90%\t\tTime 11.90s\n",
      "01/21 07:03:16 PM | Epoch[0] (14592/50000):\tLoss 0.2607\tAccurary 91.90%\t\tTime 11.90s\n",
      "01/21 07:03:16 PM | Epoch[0] (14592/50000):\tLoss 0.2607\tAccurary 91.90%\t\tTime 11.90s\n",
      "01/21 07:03:16 PM | Epoch[0] (14592/50000):\tLoss 0.2607\tAccurary 91.90%\t\tTime 11.90s\n",
      "01/21 07:03:28 PM | Epoch[0] (19456/50000):\tLoss 0.2321\tAccurary 92.81%\t\tTime 11.93s\n",
      "01/21 07:03:28 PM | Epoch[0] (19456/50000):\tLoss 0.2321\tAccurary 92.81%\t\tTime 11.93s\n",
      "01/21 07:03:28 PM | Epoch[0] (19456/50000):\tLoss 0.2321\tAccurary 92.81%\t\tTime 11.93s\n",
      "01/21 07:03:28 PM | Epoch[0] (19456/50000):\tLoss 0.2321\tAccurary 92.81%\t\tTime 11.93s\n",
      "01/21 07:03:28 PM | Epoch[0] (19456/50000):\tLoss 0.2321\tAccurary 92.81%\t\tTime 11.93s\n",
      "01/21 07:03:40 PM | Epoch[0] (24320/50000):\tLoss 0.2145\tAccurary 93.28%\t\tTime 11.90s\n",
      "01/21 07:03:40 PM | Epoch[0] (24320/50000):\tLoss 0.2145\tAccurary 93.28%\t\tTime 11.90s\n",
      "01/21 07:03:40 PM | Epoch[0] (24320/50000):\tLoss 0.2145\tAccurary 93.28%\t\tTime 11.90s\n",
      "01/21 07:03:40 PM | Epoch[0] (24320/50000):\tLoss 0.2145\tAccurary 93.28%\t\tTime 11.90s\n",
      "01/21 07:03:40 PM | Epoch[0] (24320/50000):\tLoss 0.2145\tAccurary 93.28%\t\tTime 11.90s\n",
      "01/21 07:03:52 PM | Epoch[0] (29184/50000):\tLoss 0.2065\tAccurary 93.46%\t\tTime 11.97s\n",
      "01/21 07:03:52 PM | Epoch[0] (29184/50000):\tLoss 0.2065\tAccurary 93.46%\t\tTime 11.97s\n",
      "01/21 07:03:52 PM | Epoch[0] (29184/50000):\tLoss 0.2065\tAccurary 93.46%\t\tTime 11.97s\n",
      "01/21 07:03:52 PM | Epoch[0] (29184/50000):\tLoss 0.2065\tAccurary 93.46%\t\tTime 11.97s\n",
      "01/21 07:03:52 PM | Epoch[0] (29184/50000):\tLoss 0.2065\tAccurary 93.46%\t\tTime 11.97s\n",
      "01/21 07:04:04 PM | Epoch[0] (34048/50000):\tLoss 0.2012\tAccurary 93.59%\t\tTime 11.97s\n",
      "01/21 07:04:04 PM | Epoch[0] (34048/50000):\tLoss 0.2012\tAccurary 93.59%\t\tTime 11.97s\n",
      "01/21 07:04:04 PM | Epoch[0] (34048/50000):\tLoss 0.2012\tAccurary 93.59%\t\tTime 11.97s\n",
      "01/21 07:04:04 PM | Epoch[0] (34048/50000):\tLoss 0.2012\tAccurary 93.59%\t\tTime 11.97s\n",
      "01/21 07:04:04 PM | Epoch[0] (34048/50000):\tLoss 0.2012\tAccurary 93.59%\t\tTime 11.97s\n",
      "01/21 07:04:16 PM | Epoch[0] (38912/50000):\tLoss 0.1962\tAccurary 93.73%\t\tTime 11.93s\n",
      "01/21 07:04:16 PM | Epoch[0] (38912/50000):\tLoss 0.1962\tAccurary 93.73%\t\tTime 11.93s\n",
      "01/21 07:04:16 PM | Epoch[0] (38912/50000):\tLoss 0.1962\tAccurary 93.73%\t\tTime 11.93s\n",
      "01/21 07:04:16 PM | Epoch[0] (38912/50000):\tLoss 0.1962\tAccurary 93.73%\t\tTime 11.93s\n",
      "01/21 07:04:16 PM | Epoch[0] (38912/50000):\tLoss 0.1962\tAccurary 93.73%\t\tTime 11.93s\n",
      "01/21 07:04:28 PM | Epoch[0] (43776/50000):\tLoss 0.1907\tAccurary 93.89%\t\tTime 11.92s\n",
      "01/21 07:04:28 PM | Epoch[0] (43776/50000):\tLoss 0.1907\tAccurary 93.89%\t\tTime 11.92s\n",
      "01/21 07:04:28 PM | Epoch[0] (43776/50000):\tLoss 0.1907\tAccurary 93.89%\t\tTime 11.92s\n",
      "01/21 07:04:28 PM | Epoch[0] (43776/50000):\tLoss 0.1907\tAccurary 93.89%\t\tTime 11.92s\n",
      "01/21 07:04:28 PM | Epoch[0] (43776/50000):\tLoss 0.1907\tAccurary 93.89%\t\tTime 11.92s\n",
      "01/21 07:04:40 PM | Epoch[0] (48640/50000):\tLoss 0.1871\tAccurary 93.98%\t\tTime 11.94s\n",
      "01/21 07:04:40 PM | Epoch[0] (48640/50000):\tLoss 0.1871\tAccurary 93.98%\t\tTime 11.94s\n",
      "01/21 07:04:40 PM | Epoch[0] (48640/50000):\tLoss 0.1871\tAccurary 93.98%\t\tTime 11.94s\n",
      "01/21 07:04:40 PM | Epoch[0] (48640/50000):\tLoss 0.1871\tAccurary 93.98%\t\tTime 11.94s\n",
      "01/21 07:04:40 PM | Epoch[0] (48640/50000):\tLoss 0.1871\tAccurary 93.98%\t\tTime 11.94s\n",
      "01/21 07:04:49 PM | Test Loss 0.3588\tAccurary 88.84%\t\tTime 5.80s\n",
      "\n",
      "01/21 07:04:49 PM | Test Loss 0.3588\tAccurary 88.84%\t\tTime 5.80s\n",
      "\n",
      "01/21 07:04:49 PM | Test Loss 0.3588\tAccurary 88.84%\t\tTime 5.80s\n",
      "\n",
      "01/21 07:04:49 PM | Test Loss 0.3588\tAccurary 88.84%\t\tTime 5.80s\n",
      "\n",
      "01/21 07:04:49 PM | Test Loss 0.3588\tAccurary 88.84%\t\tTime 5.80s\n",
      "\n",
      "01/21 07:05:05 PM | Epoch[1] (4864/50000):\tLoss 0.1495\tAccurary 95.14%\t\tTime 16.50s\n",
      "01/21 07:05:05 PM | Epoch[1] (4864/50000):\tLoss 0.1495\tAccurary 95.14%\t\tTime 16.50s\n",
      "01/21 07:05:05 PM | Epoch[1] (4864/50000):\tLoss 0.1495\tAccurary 95.14%\t\tTime 16.50s\n",
      "01/21 07:05:05 PM | Epoch[1] (4864/50000):\tLoss 0.1495\tAccurary 95.14%\t\tTime 16.50s\n",
      "01/21 07:05:05 PM | Epoch[1] (4864/50000):\tLoss 0.1495\tAccurary 95.14%\t\tTime 16.50s\n",
      "01/21 07:05:17 PM | Epoch[1] (9728/50000):\tLoss 0.1487\tAccurary 95.22%\t\tTime 11.93s\n",
      "01/21 07:05:17 PM | Epoch[1] (9728/50000):\tLoss 0.1487\tAccurary 95.22%\t\tTime 11.93s\n",
      "01/21 07:05:17 PM | Epoch[1] (9728/50000):\tLoss 0.1487\tAccurary 95.22%\t\tTime 11.93s\n",
      "01/21 07:05:17 PM | Epoch[1] (9728/50000):\tLoss 0.1487\tAccurary 95.22%\t\tTime 11.93s\n",
      "01/21 07:05:17 PM | Epoch[1] (9728/50000):\tLoss 0.1487\tAccurary 95.22%\t\tTime 11.93s\n",
      "01/21 07:05:29 PM | Epoch[1] (14592/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.87s\n",
      "01/21 07:05:29 PM | Epoch[1] (14592/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.87s\n",
      "01/21 07:05:29 PM | Epoch[1] (14592/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.87s\n",
      "01/21 07:05:29 PM | Epoch[1] (14592/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.87s\n",
      "01/21 07:05:29 PM | Epoch[1] (14592/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.87s\n",
      "01/21 07:05:41 PM | Epoch[1] (19456/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.89s\n",
      "01/21 07:05:41 PM | Epoch[1] (19456/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.89s\n",
      "01/21 07:05:41 PM | Epoch[1] (19456/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.89s\n",
      "01/21 07:05:41 PM | Epoch[1] (19456/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.89s\n",
      "01/21 07:05:41 PM | Epoch[1] (19456/50000):\tLoss 0.1472\tAccurary 95.17%\t\tTime 11.89s\n",
      "01/21 07:05:53 PM | Epoch[1] (24320/50000):\tLoss 0.1471\tAccurary 95.19%\t\tTime 11.91s\n",
      "01/21 07:05:53 PM | Epoch[1] (24320/50000):\tLoss 0.1471\tAccurary 95.19%\t\tTime 11.91s\n",
      "01/21 07:05:53 PM | Epoch[1] (24320/50000):\tLoss 0.1471\tAccurary 95.19%\t\tTime 11.91s\n",
      "01/21 07:05:53 PM | Epoch[1] (24320/50000):\tLoss 0.1471\tAccurary 95.19%\t\tTime 11.91s\n",
      "01/21 07:05:53 PM | Epoch[1] (24320/50000):\tLoss 0.1471\tAccurary 95.19%\t\tTime 11.91s\n",
      "01/21 07:06:04 PM | Epoch[1] (29184/50000):\tLoss 0.1488\tAccurary 95.15%\t\tTime 11.86s\n",
      "01/21 07:06:04 PM | Epoch[1] (29184/50000):\tLoss 0.1488\tAccurary 95.15%\t\tTime 11.86s\n",
      "01/21 07:06:04 PM | Epoch[1] (29184/50000):\tLoss 0.1488\tAccurary 95.15%\t\tTime 11.86s\n",
      "01/21 07:06:04 PM | Epoch[1] (29184/50000):\tLoss 0.1488\tAccurary 95.15%\t\tTime 11.86s\n",
      "01/21 07:06:04 PM | Epoch[1] (29184/50000):\tLoss 0.1488\tAccurary 95.15%\t\tTime 11.86s\n",
      "01/21 07:06:16 PM | Epoch[1] (34048/50000):\tLoss 0.1508\tAccurary 95.08%\t\tTime 11.87s\n",
      "01/21 07:06:16 PM | Epoch[1] (34048/50000):\tLoss 0.1508\tAccurary 95.08%\t\tTime 11.87s\n",
      "01/21 07:06:16 PM | Epoch[1] (34048/50000):\tLoss 0.1508\tAccurary 95.08%\t\tTime 11.87s\n",
      "01/21 07:06:16 PM | Epoch[1] (34048/50000):\tLoss 0.1508\tAccurary 95.08%\t\tTime 11.87s\n",
      "01/21 07:06:16 PM | Epoch[1] (34048/50000):\tLoss 0.1508\tAccurary 95.08%\t\tTime 11.87s\n",
      "01/21 07:06:28 PM | Epoch[1] (38912/50000):\tLoss 0.1543\tAccurary 94.94%\t\tTime 11.87s\n",
      "01/21 07:06:28 PM | Epoch[1] (38912/50000):\tLoss 0.1543\tAccurary 94.94%\t\tTime 11.87s\n",
      "01/21 07:06:28 PM | Epoch[1] (38912/50000):\tLoss 0.1543\tAccurary 94.94%\t\tTime 11.87s\n",
      "01/21 07:06:28 PM | Epoch[1] (38912/50000):\tLoss 0.1543\tAccurary 94.94%\t\tTime 11.87s\n",
      "01/21 07:06:28 PM | Epoch[1] (38912/50000):\tLoss 0.1543\tAccurary 94.94%\t\tTime 11.87s\n",
      "01/21 07:06:40 PM | Epoch[1] (43776/50000):\tLoss 0.1543\tAccurary 94.97%\t\tTime 11.88s\n",
      "01/21 07:06:40 PM | Epoch[1] (43776/50000):\tLoss 0.1543\tAccurary 94.97%\t\tTime 11.88s\n",
      "01/21 07:06:40 PM | Epoch[1] (43776/50000):\tLoss 0.1543\tAccurary 94.97%\t\tTime 11.88s\n",
      "01/21 07:06:40 PM | Epoch[1] (43776/50000):\tLoss 0.1543\tAccurary 94.97%\t\tTime 11.88s\n",
      "01/21 07:06:40 PM | Epoch[1] (43776/50000):\tLoss 0.1543\tAccurary 94.97%\t\tTime 11.88s\n",
      "01/21 07:06:52 PM | Epoch[1] (48640/50000):\tLoss 0.1548\tAccurary 94.99%\t\tTime 11.90s\n",
      "01/21 07:06:52 PM | Epoch[1] (48640/50000):\tLoss 0.1548\tAccurary 94.99%\t\tTime 11.90s\n",
      "01/21 07:06:52 PM | Epoch[1] (48640/50000):\tLoss 0.1548\tAccurary 94.99%\t\tTime 11.90s\n",
      "01/21 07:06:52 PM | Epoch[1] (48640/50000):\tLoss 0.1548\tAccurary 94.99%\t\tTime 11.90s\n",
      "01/21 07:06:52 PM | Epoch[1] (48640/50000):\tLoss 0.1548\tAccurary 94.99%\t\tTime 11.90s\n",
      "01/21 07:07:01 PM | Test Loss 0.4153\tAccurary 86.59%\t\tTime 5.90s\n",
      "\n",
      "01/21 07:07:01 PM | Test Loss 0.4153\tAccurary 86.59%\t\tTime 5.90s\n",
      "\n",
      "01/21 07:07:01 PM | Test Loss 0.4153\tAccurary 86.59%\t\tTime 5.90s\n",
      "\n",
      "01/21 07:07:01 PM | Test Loss 0.4153\tAccurary 86.59%\t\tTime 5.90s\n",
      "\n",
      "01/21 07:07:01 PM | Test Loss 0.4153\tAccurary 86.59%\t\tTime 5.90s\n",
      "\n",
      "01/21 07:07:01 PM | ############# model4_2epoch:88.83999633789062 using 264.77026557922363s #################\n",
      "01/21 07:07:01 PM | ############# model4_2epoch:88.83999633789062 using 264.77026557922363s #################\n",
      "01/21 07:07:01 PM | ############# model4_2epoch:88.83999633789062 using 264.77026557922363s #################\n",
      "01/21 07:07:01 PM | ############# model4_2epoch:88.83999633789062 using 264.77026557922363s #################\n",
      "01/21 07:07:01 PM | ############# model4_2epoch:88.83999633789062 using 264.77026557922363s #################\n",
      "01/21 07:07:33 PM | Epoch[0] (4864/50000):\tLoss 0.5501\tAccurary 84.00%\t\tTime 32.00s\n",
      "01/21 07:07:33 PM | Epoch[0] (4864/50000):\tLoss 0.5501\tAccurary 84.00%\t\tTime 32.00s\n",
      "01/21 07:07:33 PM | Epoch[0] (4864/50000):\tLoss 0.5501\tAccurary 84.00%\t\tTime 32.00s\n",
      "01/21 07:07:33 PM | Epoch[0] (4864/50000):\tLoss 0.5501\tAccurary 84.00%\t\tTime 32.00s\n",
      "01/21 07:07:33 PM | Epoch[0] (4864/50000):\tLoss 0.5501\tAccurary 84.00%\t\tTime 32.00s\n",
      "01/21 07:07:57 PM | Epoch[0] (9728/50000):\tLoss 0.4038\tAccurary 87.80%\t\tTime 23.84s\n",
      "01/21 07:07:57 PM | Epoch[0] (9728/50000):\tLoss 0.4038\tAccurary 87.80%\t\tTime 23.84s\n",
      "01/21 07:07:57 PM | Epoch[0] (9728/50000):\tLoss 0.4038\tAccurary 87.80%\t\tTime 23.84s\n",
      "01/21 07:07:57 PM | Epoch[0] (9728/50000):\tLoss 0.4038\tAccurary 87.80%\t\tTime 23.84s\n",
      "01/21 07:07:57 PM | Epoch[0] (9728/50000):\tLoss 0.4038\tAccurary 87.80%\t\tTime 23.84s\n",
      "01/21 07:08:08 PM | Epoch[0] (14592/50000):\tLoss 0.3323\tAccurary 89.86%\t\tTime 11.41s\n",
      "01/21 07:08:08 PM | Epoch[0] (14592/50000):\tLoss 0.3323\tAccurary 89.86%\t\tTime 11.41s\n",
      "01/21 07:08:08 PM | Epoch[0] (14592/50000):\tLoss 0.3323\tAccurary 89.86%\t\tTime 11.41s\n",
      "01/21 07:08:08 PM | Epoch[0] (14592/50000):\tLoss 0.3323\tAccurary 89.86%\t\tTime 11.41s\n",
      "01/21 07:08:08 PM | Epoch[0] (14592/50000):\tLoss 0.3323\tAccurary 89.86%\t\tTime 11.41s\n",
      "01/21 07:08:20 PM | Epoch[0] (19456/50000):\tLoss 0.2988\tAccurary 90.77%\t\tTime 11.35s\n",
      "01/21 07:08:20 PM | Epoch[0] (19456/50000):\tLoss 0.2988\tAccurary 90.77%\t\tTime 11.35s\n",
      "01/21 07:08:20 PM | Epoch[0] (19456/50000):\tLoss 0.2988\tAccurary 90.77%\t\tTime 11.35s\n",
      "01/21 07:08:20 PM | Epoch[0] (19456/50000):\tLoss 0.2988\tAccurary 90.77%\t\tTime 11.35s\n",
      "01/21 07:08:20 PM | Epoch[0] (19456/50000):\tLoss 0.2988\tAccurary 90.77%\t\tTime 11.35s\n",
      "01/21 07:08:31 PM | Epoch[0] (24320/50000):\tLoss 0.2770\tAccurary 91.33%\t\tTime 11.00s\n",
      "01/21 07:08:31 PM | Epoch[0] (24320/50000):\tLoss 0.2770\tAccurary 91.33%\t\tTime 11.00s\n",
      "01/21 07:08:31 PM | Epoch[0] (24320/50000):\tLoss 0.2770\tAccurary 91.33%\t\tTime 11.00s\n",
      "01/21 07:08:31 PM | Epoch[0] (24320/50000):\tLoss 0.2770\tAccurary 91.33%\t\tTime 11.00s\n",
      "01/21 07:08:31 PM | Epoch[0] (24320/50000):\tLoss 0.2770\tAccurary 91.33%\t\tTime 11.00s\n",
      "01/21 07:08:42 PM | Epoch[0] (29184/50000):\tLoss 0.2668\tAccurary 91.58%\t\tTime 10.92s\n",
      "01/21 07:08:42 PM | Epoch[0] (29184/50000):\tLoss 0.2668\tAccurary 91.58%\t\tTime 10.92s\n",
      "01/21 07:08:42 PM | Epoch[0] (29184/50000):\tLoss 0.2668\tAccurary 91.58%\t\tTime 10.92s\n",
      "01/21 07:08:42 PM | Epoch[0] (29184/50000):\tLoss 0.2668\tAccurary 91.58%\t\tTime 10.92s\n",
      "01/21 07:08:42 PM | Epoch[0] (29184/50000):\tLoss 0.2668\tAccurary 91.58%\t\tTime 10.92s\n",
      "01/21 07:08:53 PM | Epoch[0] (34048/50000):\tLoss 0.2535\tAccurary 92.00%\t\tTime 11.01s\n",
      "01/21 07:08:53 PM | Epoch[0] (34048/50000):\tLoss 0.2535\tAccurary 92.00%\t\tTime 11.01s\n",
      "01/21 07:08:53 PM | Epoch[0] (34048/50000):\tLoss 0.2535\tAccurary 92.00%\t\tTime 11.01s\n",
      "01/21 07:08:53 PM | Epoch[0] (34048/50000):\tLoss 0.2535\tAccurary 92.00%\t\tTime 11.01s\n",
      "01/21 07:08:53 PM | Epoch[0] (34048/50000):\tLoss 0.2535\tAccurary 92.00%\t\tTime 11.01s\n",
      "01/21 07:09:04 PM | Epoch[0] (38912/50000):\tLoss 0.2459\tAccurary 92.24%\t\tTime 10.93s\n",
      "01/21 07:09:04 PM | Epoch[0] (38912/50000):\tLoss 0.2459\tAccurary 92.24%\t\tTime 10.93s\n",
      "01/21 07:09:04 PM | Epoch[0] (38912/50000):\tLoss 0.2459\tAccurary 92.24%\t\tTime 10.93s\n",
      "01/21 07:09:04 PM | Epoch[0] (38912/50000):\tLoss 0.2459\tAccurary 92.24%\t\tTime 10.93s\n",
      "01/21 07:09:04 PM | Epoch[0] (38912/50000):\tLoss 0.2459\tAccurary 92.24%\t\tTime 10.93s\n",
      "01/21 07:09:14 PM | Epoch[0] (43776/50000):\tLoss 0.2405\tAccurary 92.39%\t\tTime 10.77s\n",
      "01/21 07:09:14 PM | Epoch[0] (43776/50000):\tLoss 0.2405\tAccurary 92.39%\t\tTime 10.77s\n",
      "01/21 07:09:14 PM | Epoch[0] (43776/50000):\tLoss 0.2405\tAccurary 92.39%\t\tTime 10.77s\n",
      "01/21 07:09:14 PM | Epoch[0] (43776/50000):\tLoss 0.2405\tAccurary 92.39%\t\tTime 10.77s\n",
      "01/21 07:09:14 PM | Epoch[0] (43776/50000):\tLoss 0.2405\tAccurary 92.39%\t\tTime 10.77s\n",
      "01/21 07:09:25 PM | Epoch[0] (48640/50000):\tLoss 0.2374\tAccurary 92.42%\t\tTime 10.79s\n",
      "01/21 07:09:25 PM | Epoch[0] (48640/50000):\tLoss 0.2374\tAccurary 92.42%\t\tTime 10.79s\n",
      "01/21 07:09:25 PM | Epoch[0] (48640/50000):\tLoss 0.2374\tAccurary 92.42%\t\tTime 10.79s\n",
      "01/21 07:09:25 PM | Epoch[0] (48640/50000):\tLoss 0.2374\tAccurary 92.42%\t\tTime 10.79s\n",
      "01/21 07:09:25 PM | Epoch[0] (48640/50000):\tLoss 0.2374\tAccurary 92.42%\t\tTime 10.79s\n",
      "01/21 07:09:34 PM | Test Loss 0.3981\tAccurary 87.54%\t\tTime 6.77s\n",
      "\n",
      "01/21 07:09:34 PM | Test Loss 0.3981\tAccurary 87.54%\t\tTime 6.77s\n",
      "\n",
      "01/21 07:09:34 PM | Test Loss 0.3981\tAccurary 87.54%\t\tTime 6.77s\n",
      "\n",
      "01/21 07:09:34 PM | Test Loss 0.3981\tAccurary 87.54%\t\tTime 6.77s\n",
      "\n",
      "01/21 07:09:34 PM | Test Loss 0.3981\tAccurary 87.54%\t\tTime 6.77s\n",
      "\n",
      "01/21 07:09:50 PM | Epoch[1] (4864/50000):\tLoss 0.1652\tAccurary 94.45%\t\tTime 15.20s\n",
      "01/21 07:09:50 PM | Epoch[1] (4864/50000):\tLoss 0.1652\tAccurary 94.45%\t\tTime 15.20s\n",
      "01/21 07:09:50 PM | Epoch[1] (4864/50000):\tLoss 0.1652\tAccurary 94.45%\t\tTime 15.20s\n",
      "01/21 07:09:50 PM | Epoch[1] (4864/50000):\tLoss 0.1652\tAccurary 94.45%\t\tTime 15.20s\n",
      "01/21 07:09:50 PM | Epoch[1] (4864/50000):\tLoss 0.1652\tAccurary 94.45%\t\tTime 15.20s\n",
      "01/21 07:10:00 PM | Epoch[1] (9728/50000):\tLoss 0.1684\tAccurary 94.34%\t\tTime 10.75s\n",
      "01/21 07:10:00 PM | Epoch[1] (9728/50000):\tLoss 0.1684\tAccurary 94.34%\t\tTime 10.75s\n",
      "01/21 07:10:00 PM | Epoch[1] (9728/50000):\tLoss 0.1684\tAccurary 94.34%\t\tTime 10.75s\n",
      "01/21 07:10:00 PM | Epoch[1] (9728/50000):\tLoss 0.1684\tAccurary 94.34%\t\tTime 10.75s\n",
      "01/21 07:10:00 PM | Epoch[1] (9728/50000):\tLoss 0.1684\tAccurary 94.34%\t\tTime 10.75s\n",
      "01/21 07:10:11 PM | Epoch[1] (14592/50000):\tLoss 0.1693\tAccurary 94.17%\t\tTime 10.78s\n",
      "01/21 07:10:11 PM | Epoch[1] (14592/50000):\tLoss 0.1693\tAccurary 94.17%\t\tTime 10.78s\n",
      "01/21 07:10:11 PM | Epoch[1] (14592/50000):\tLoss 0.1693\tAccurary 94.17%\t\tTime 10.78s\n",
      "01/21 07:10:11 PM | Epoch[1] (14592/50000):\tLoss 0.1693\tAccurary 94.17%\t\tTime 10.78s\n",
      "01/21 07:10:11 PM | Epoch[1] (14592/50000):\tLoss 0.1693\tAccurary 94.17%\t\tTime 10.78s\n",
      "01/21 07:10:22 PM | Epoch[1] (19456/50000):\tLoss 0.1713\tAccurary 94.15%\t\tTime 10.77s\n",
      "01/21 07:10:22 PM | Epoch[1] (19456/50000):\tLoss 0.1713\tAccurary 94.15%\t\tTime 10.77s\n",
      "01/21 07:10:22 PM | Epoch[1] (19456/50000):\tLoss 0.1713\tAccurary 94.15%\t\tTime 10.77s\n",
      "01/21 07:10:22 PM | Epoch[1] (19456/50000):\tLoss 0.1713\tAccurary 94.15%\t\tTime 10.77s\n",
      "01/21 07:10:22 PM | Epoch[1] (19456/50000):\tLoss 0.1713\tAccurary 94.15%\t\tTime 10.77s\n",
      "01/21 07:10:33 PM | Epoch[1] (24320/50000):\tLoss 0.1735\tAccurary 94.09%\t\tTime 10.76s\n",
      "01/21 07:10:33 PM | Epoch[1] (24320/50000):\tLoss 0.1735\tAccurary 94.09%\t\tTime 10.76s\n",
      "01/21 07:10:33 PM | Epoch[1] (24320/50000):\tLoss 0.1735\tAccurary 94.09%\t\tTime 10.76s\n",
      "01/21 07:10:33 PM | Epoch[1] (24320/50000):\tLoss 0.1735\tAccurary 94.09%\t\tTime 10.76s\n",
      "01/21 07:10:33 PM | Epoch[1] (24320/50000):\tLoss 0.1735\tAccurary 94.09%\t\tTime 10.76s\n",
      "01/21 07:10:43 PM | Epoch[1] (29184/50000):\tLoss 0.1745\tAccurary 94.10%\t\tTime 10.78s\n",
      "01/21 07:10:43 PM | Epoch[1] (29184/50000):\tLoss 0.1745\tAccurary 94.10%\t\tTime 10.78s\n",
      "01/21 07:10:43 PM | Epoch[1] (29184/50000):\tLoss 0.1745\tAccurary 94.10%\t\tTime 10.78s\n",
      "01/21 07:10:43 PM | Epoch[1] (29184/50000):\tLoss 0.1745\tAccurary 94.10%\t\tTime 10.78s\n",
      "01/21 07:10:43 PM | Epoch[1] (29184/50000):\tLoss 0.1745\tAccurary 94.10%\t\tTime 10.78s\n",
      "01/21 07:10:54 PM | Epoch[1] (34048/50000):\tLoss 0.1761\tAccurary 94.04%\t\tTime 10.74s\n",
      "01/21 07:10:54 PM | Epoch[1] (34048/50000):\tLoss 0.1761\tAccurary 94.04%\t\tTime 10.74s\n",
      "01/21 07:10:54 PM | Epoch[1] (34048/50000):\tLoss 0.1761\tAccurary 94.04%\t\tTime 10.74s\n",
      "01/21 07:10:54 PM | Epoch[1] (34048/50000):\tLoss 0.1761\tAccurary 94.04%\t\tTime 10.74s\n",
      "01/21 07:10:54 PM | Epoch[1] (34048/50000):\tLoss 0.1761\tAccurary 94.04%\t\tTime 10.74s\n",
      "01/21 07:11:05 PM | Epoch[1] (38912/50000):\tLoss 0.1782\tAccurary 94.01%\t\tTime 10.76s\n",
      "01/21 07:11:05 PM | Epoch[1] (38912/50000):\tLoss 0.1782\tAccurary 94.01%\t\tTime 10.76s\n",
      "01/21 07:11:05 PM | Epoch[1] (38912/50000):\tLoss 0.1782\tAccurary 94.01%\t\tTime 10.76s\n",
      "01/21 07:11:05 PM | Epoch[1] (38912/50000):\tLoss 0.1782\tAccurary 94.01%\t\tTime 10.76s\n",
      "01/21 07:11:05 PM | Epoch[1] (38912/50000):\tLoss 0.1782\tAccurary 94.01%\t\tTime 10.76s\n",
      "01/21 07:11:16 PM | Epoch[1] (43776/50000):\tLoss 0.1779\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:16 PM | Epoch[1] (43776/50000):\tLoss 0.1779\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:16 PM | Epoch[1] (43776/50000):\tLoss 0.1779\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:16 PM | Epoch[1] (43776/50000):\tLoss 0.1779\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:16 PM | Epoch[1] (43776/50000):\tLoss 0.1779\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:27 PM | Epoch[1] (48640/50000):\tLoss 0.1777\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:27 PM | Epoch[1] (48640/50000):\tLoss 0.1777\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:27 PM | Epoch[1] (48640/50000):\tLoss 0.1777\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:27 PM | Epoch[1] (48640/50000):\tLoss 0.1777\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:27 PM | Epoch[1] (48640/50000):\tLoss 0.1777\tAccurary 94.01%\t\tTime 10.77s\n",
      "01/21 07:11:36 PM | Test Loss 0.4057\tAccurary 87.58%\t\tTime 6.89s\n",
      "\n",
      "01/21 07:11:36 PM | Test Loss 0.4057\tAccurary 87.58%\t\tTime 6.89s\n",
      "\n",
      "01/21 07:11:36 PM | Test Loss 0.4057\tAccurary 87.58%\t\tTime 6.89s\n",
      "\n",
      "01/21 07:11:36 PM | Test Loss 0.4057\tAccurary 87.58%\t\tTime 6.89s\n",
      "\n",
      "01/21 07:11:36 PM | Test Loss 0.4057\tAccurary 87.58%\t\tTime 6.89s\n",
      "\n",
      "01/21 07:11:36 PM | ############# model5_2epoch:87.57999420166016 using 274.877277135849s #################\n",
      "01/21 07:11:36 PM | ############# model5_2epoch:87.57999420166016 using 274.877277135849s #################\n",
      "01/21 07:11:36 PM | ############# model5_2epoch:87.57999420166016 using 274.877277135849s #################\n",
      "01/21 07:11:36 PM | ############# model5_2epoch:87.57999420166016 using 274.877277135849s #################\n",
      "01/21 07:11:36 PM | ############# model5_2epoch:87.57999420166016 using 274.877277135849s #################\n",
      "01/21 07:11:48 PM | Epoch[0] (4864/50000):\tLoss 0.8561\tAccurary 78.26%\t\tTime 12.23s\n",
      "01/21 07:11:48 PM | Epoch[0] (4864/50000):\tLoss 0.8561\tAccurary 78.26%\t\tTime 12.23s\n",
      "01/21 07:11:48 PM | Epoch[0] (4864/50000):\tLoss 0.8561\tAccurary 78.26%\t\tTime 12.23s\n",
      "01/21 07:11:48 PM | Epoch[0] (4864/50000):\tLoss 0.8561\tAccurary 78.26%\t\tTime 12.23s\n",
      "01/21 07:11:48 PM | Epoch[0] (4864/50000):\tLoss 0.8561\tAccurary 78.26%\t\tTime 12.23s\n",
      "01/21 07:11:56 PM | Epoch[0] (9728/50000):\tLoss 0.5879\tAccurary 85.13%\t\tTime 7.98s\n",
      "01/21 07:11:56 PM | Epoch[0] (9728/50000):\tLoss 0.5879\tAccurary 85.13%\t\tTime 7.98s\n",
      "01/21 07:11:56 PM | Epoch[0] (9728/50000):\tLoss 0.5879\tAccurary 85.13%\t\tTime 7.98s\n",
      "01/21 07:11:56 PM | Epoch[0] (9728/50000):\tLoss 0.5879\tAccurary 85.13%\t\tTime 7.98s\n",
      "01/21 07:11:56 PM | Epoch[0] (9728/50000):\tLoss 0.5879\tAccurary 85.13%\t\tTime 7.98s\n",
      "01/21 07:12:04 PM | Epoch[0] (14592/50000):\tLoss 0.4667\tAccurary 87.98%\t\tTime 7.98s\n",
      "01/21 07:12:04 PM | Epoch[0] (14592/50000):\tLoss 0.4667\tAccurary 87.98%\t\tTime 7.98s\n",
      "01/21 07:12:04 PM | Epoch[0] (14592/50000):\tLoss 0.4667\tAccurary 87.98%\t\tTime 7.98s\n",
      "01/21 07:12:04 PM | Epoch[0] (14592/50000):\tLoss 0.4667\tAccurary 87.98%\t\tTime 7.98s\n",
      "01/21 07:12:04 PM | Epoch[0] (14592/50000):\tLoss 0.4667\tAccurary 87.98%\t\tTime 7.98s\n",
      "01/21 07:12:12 PM | Epoch[0] (19456/50000):\tLoss 0.4024\tAccurary 89.42%\t\tTime 8.06s\n",
      "01/21 07:12:12 PM | Epoch[0] (19456/50000):\tLoss 0.4024\tAccurary 89.42%\t\tTime 8.06s\n",
      "01/21 07:12:12 PM | Epoch[0] (19456/50000):\tLoss 0.4024\tAccurary 89.42%\t\tTime 8.06s\n",
      "01/21 07:12:12 PM | Epoch[0] (19456/50000):\tLoss 0.4024\tAccurary 89.42%\t\tTime 8.06s\n",
      "01/21 07:12:12 PM | Epoch[0] (19456/50000):\tLoss 0.4024\tAccurary 89.42%\t\tTime 8.06s\n",
      "01/21 07:12:20 PM | Epoch[0] (24320/50000):\tLoss 0.3640\tAccurary 90.20%\t\tTime 7.97s\n",
      "01/21 07:12:20 PM | Epoch[0] (24320/50000):\tLoss 0.3640\tAccurary 90.20%\t\tTime 7.97s\n",
      "01/21 07:12:20 PM | Epoch[0] (24320/50000):\tLoss 0.3640\tAccurary 90.20%\t\tTime 7.97s\n",
      "01/21 07:12:20 PM | Epoch[0] (24320/50000):\tLoss 0.3640\tAccurary 90.20%\t\tTime 7.97s\n",
      "01/21 07:12:20 PM | Epoch[0] (24320/50000):\tLoss 0.3640\tAccurary 90.20%\t\tTime 7.97s\n",
      "01/21 07:12:28 PM | Epoch[0] (29184/50000):\tLoss 0.3386\tAccurary 90.68%\t\tTime 7.99s\n",
      "01/21 07:12:28 PM | Epoch[0] (29184/50000):\tLoss 0.3386\tAccurary 90.68%\t\tTime 7.99s\n",
      "01/21 07:12:28 PM | Epoch[0] (29184/50000):\tLoss 0.3386\tAccurary 90.68%\t\tTime 7.99s\n",
      "01/21 07:12:28 PM | Epoch[0] (29184/50000):\tLoss 0.3386\tAccurary 90.68%\t\tTime 7.99s\n",
      "01/21 07:12:28 PM | Epoch[0] (29184/50000):\tLoss 0.3386\tAccurary 90.68%\t\tTime 7.99s\n",
      "01/21 07:12:36 PM | Epoch[0] (34048/50000):\tLoss 0.3188\tAccurary 91.07%\t\tTime 7.98s\n",
      "01/21 07:12:36 PM | Epoch[0] (34048/50000):\tLoss 0.3188\tAccurary 91.07%\t\tTime 7.98s\n",
      "01/21 07:12:36 PM | Epoch[0] (34048/50000):\tLoss 0.3188\tAccurary 91.07%\t\tTime 7.98s\n",
      "01/21 07:12:36 PM | Epoch[0] (34048/50000):\tLoss 0.3188\tAccurary 91.07%\t\tTime 7.98s\n",
      "01/21 07:12:36 PM | Epoch[0] (34048/50000):\tLoss 0.3188\tAccurary 91.07%\t\tTime 7.98s\n",
      "01/21 07:12:44 PM | Epoch[0] (38912/50000):\tLoss 0.3051\tAccurary 91.32%\t\tTime 7.97s\n",
      "01/21 07:12:44 PM | Epoch[0] (38912/50000):\tLoss 0.3051\tAccurary 91.32%\t\tTime 7.97s\n",
      "01/21 07:12:44 PM | Epoch[0] (38912/50000):\tLoss 0.3051\tAccurary 91.32%\t\tTime 7.97s\n",
      "01/21 07:12:44 PM | Epoch[0] (38912/50000):\tLoss 0.3051\tAccurary 91.32%\t\tTime 7.97s\n",
      "01/21 07:12:44 PM | Epoch[0] (38912/50000):\tLoss 0.3051\tAccurary 91.32%\t\tTime 7.97s\n",
      "01/21 07:12:52 PM | Epoch[0] (43776/50000):\tLoss 0.2930\tAccurary 91.56%\t\tTime 8.07s\n",
      "01/21 07:12:52 PM | Epoch[0] (43776/50000):\tLoss 0.2930\tAccurary 91.56%\t\tTime 8.07s\n",
      "01/21 07:12:52 PM | Epoch[0] (43776/50000):\tLoss 0.2930\tAccurary 91.56%\t\tTime 8.07s\n",
      "01/21 07:12:52 PM | Epoch[0] (43776/50000):\tLoss 0.2930\tAccurary 91.56%\t\tTime 8.07s\n",
      "01/21 07:12:52 PM | Epoch[0] (43776/50000):\tLoss 0.2930\tAccurary 91.56%\t\tTime 8.07s\n",
      "01/21 07:13:00 PM | Epoch[0] (48640/50000):\tLoss 0.2843\tAccurary 91.73%\t\tTime 7.98s\n",
      "01/21 07:13:00 PM | Epoch[0] (48640/50000):\tLoss 0.2843\tAccurary 91.73%\t\tTime 7.98s\n",
      "01/21 07:13:00 PM | Epoch[0] (48640/50000):\tLoss 0.2843\tAccurary 91.73%\t\tTime 7.98s\n",
      "01/21 07:13:00 PM | Epoch[0] (48640/50000):\tLoss 0.2843\tAccurary 91.73%\t\tTime 7.98s\n",
      "01/21 07:13:00 PM | Epoch[0] (48640/50000):\tLoss 0.2843\tAccurary 91.73%\t\tTime 7.98s\n",
      "01/21 07:13:07 PM | Test Loss 0.4850\tAccurary 85.01%\t\tTime 5.05s\n",
      "\n",
      "01/21 07:13:07 PM | Test Loss 0.4850\tAccurary 85.01%\t\tTime 5.05s\n",
      "\n",
      "01/21 07:13:07 PM | Test Loss 0.4850\tAccurary 85.01%\t\tTime 5.05s\n",
      "\n",
      "01/21 07:13:07 PM | Test Loss 0.4850\tAccurary 85.01%\t\tTime 5.05s\n",
      "\n",
      "01/21 07:13:07 PM | Test Loss 0.4850\tAccurary 85.01%\t\tTime 5.05s\n",
      "\n",
      "01/21 07:13:20 PM | Epoch[1] (4864/50000):\tLoss 0.1702\tAccurary 94.57%\t\tTime 12.99s\n",
      "01/21 07:13:20 PM | Epoch[1] (4864/50000):\tLoss 0.1702\tAccurary 94.57%\t\tTime 12.99s\n",
      "01/21 07:13:20 PM | Epoch[1] (4864/50000):\tLoss 0.1702\tAccurary 94.57%\t\tTime 12.99s\n",
      "01/21 07:13:20 PM | Epoch[1] (4864/50000):\tLoss 0.1702\tAccurary 94.57%\t\tTime 12.99s\n",
      "01/21 07:13:20 PM | Epoch[1] (4864/50000):\tLoss 0.1702\tAccurary 94.57%\t\tTime 12.99s\n",
      "01/21 07:13:28 PM | Epoch[1] (9728/50000):\tLoss 0.1710\tAccurary 94.67%\t\tTime 8.08s\n",
      "01/21 07:13:28 PM | Epoch[1] (9728/50000):\tLoss 0.1710\tAccurary 94.67%\t\tTime 8.08s\n",
      "01/21 07:13:28 PM | Epoch[1] (9728/50000):\tLoss 0.1710\tAccurary 94.67%\t\tTime 8.08s\n",
      "01/21 07:13:28 PM | Epoch[1] (9728/50000):\tLoss 0.1710\tAccurary 94.67%\t\tTime 8.08s\n",
      "01/21 07:13:28 PM | Epoch[1] (9728/50000):\tLoss 0.1710\tAccurary 94.67%\t\tTime 8.08s\n",
      "01/21 07:13:36 PM | Epoch[1] (14592/50000):\tLoss 0.1702\tAccurary 94.62%\t\tTime 8.04s\n",
      "01/21 07:13:36 PM | Epoch[1] (14592/50000):\tLoss 0.1702\tAccurary 94.62%\t\tTime 8.04s\n",
      "01/21 07:13:36 PM | Epoch[1] (14592/50000):\tLoss 0.1702\tAccurary 94.62%\t\tTime 8.04s\n",
      "01/21 07:13:36 PM | Epoch[1] (14592/50000):\tLoss 0.1702\tAccurary 94.62%\t\tTime 8.04s\n",
      "01/21 07:13:36 PM | Epoch[1] (14592/50000):\tLoss 0.1702\tAccurary 94.62%\t\tTime 8.04s\n",
      "01/21 07:13:45 PM | Epoch[1] (19456/50000):\tLoss 0.1746\tAccurary 94.37%\t\tTime 8.04s\n",
      "01/21 07:13:45 PM | Epoch[1] (19456/50000):\tLoss 0.1746\tAccurary 94.37%\t\tTime 8.04s\n",
      "01/21 07:13:45 PM | Epoch[1] (19456/50000):\tLoss 0.1746\tAccurary 94.37%\t\tTime 8.04s\n",
      "01/21 07:13:45 PM | Epoch[1] (19456/50000):\tLoss 0.1746\tAccurary 94.37%\t\tTime 8.04s\n",
      "01/21 07:13:45 PM | Epoch[1] (19456/50000):\tLoss 0.1746\tAccurary 94.37%\t\tTime 8.04s\n",
      "01/21 07:13:53 PM | Epoch[1] (24320/50000):\tLoss 0.1774\tAccurary 94.30%\t\tTime 8.28s\n",
      "01/21 07:13:53 PM | Epoch[1] (24320/50000):\tLoss 0.1774\tAccurary 94.30%\t\tTime 8.28s\n",
      "01/21 07:13:53 PM | Epoch[1] (24320/50000):\tLoss 0.1774\tAccurary 94.30%\t\tTime 8.28s\n",
      "01/21 07:13:53 PM | Epoch[1] (24320/50000):\tLoss 0.1774\tAccurary 94.30%\t\tTime 8.28s\n",
      "01/21 07:13:53 PM | Epoch[1] (24320/50000):\tLoss 0.1774\tAccurary 94.30%\t\tTime 8.28s\n",
      "01/21 07:14:01 PM | Epoch[1] (29184/50000):\tLoss 0.1806\tAccurary 94.21%\t\tTime 8.19s\n",
      "01/21 07:14:01 PM | Epoch[1] (29184/50000):\tLoss 0.1806\tAccurary 94.21%\t\tTime 8.19s\n",
      "01/21 07:14:01 PM | Epoch[1] (29184/50000):\tLoss 0.1806\tAccurary 94.21%\t\tTime 8.19s\n",
      "01/21 07:14:01 PM | Epoch[1] (29184/50000):\tLoss 0.1806\tAccurary 94.21%\t\tTime 8.19s\n",
      "01/21 07:14:01 PM | Epoch[1] (29184/50000):\tLoss 0.1806\tAccurary 94.21%\t\tTime 8.19s\n",
      "01/21 07:14:09 PM | Epoch[1] (34048/50000):\tLoss 0.1830\tAccurary 94.05%\t\tTime 8.16s\n",
      "01/21 07:14:09 PM | Epoch[1] (34048/50000):\tLoss 0.1830\tAccurary 94.05%\t\tTime 8.16s\n",
      "01/21 07:14:09 PM | Epoch[1] (34048/50000):\tLoss 0.1830\tAccurary 94.05%\t\tTime 8.16s\n",
      "01/21 07:14:09 PM | Epoch[1] (34048/50000):\tLoss 0.1830\tAccurary 94.05%\t\tTime 8.16s\n",
      "01/21 07:14:09 PM | Epoch[1] (34048/50000):\tLoss 0.1830\tAccurary 94.05%\t\tTime 8.16s\n",
      "01/21 07:14:17 PM | Epoch[1] (38912/50000):\tLoss 0.1861\tAccurary 93.93%\t\tTime 8.07s\n",
      "01/21 07:14:17 PM | Epoch[1] (38912/50000):\tLoss 0.1861\tAccurary 93.93%\t\tTime 8.07s\n",
      "01/21 07:14:17 PM | Epoch[1] (38912/50000):\tLoss 0.1861\tAccurary 93.93%\t\tTime 8.07s\n",
      "01/21 07:14:17 PM | Epoch[1] (38912/50000):\tLoss 0.1861\tAccurary 93.93%\t\tTime 8.07s\n",
      "01/21 07:14:17 PM | Epoch[1] (38912/50000):\tLoss 0.1861\tAccurary 93.93%\t\tTime 8.07s\n",
      "01/21 07:14:25 PM | Epoch[1] (43776/50000):\tLoss 0.1887\tAccurary 93.83%\t\tTime 8.21s\n",
      "01/21 07:14:25 PM | Epoch[1] (43776/50000):\tLoss 0.1887\tAccurary 93.83%\t\tTime 8.21s\n",
      "01/21 07:14:25 PM | Epoch[1] (43776/50000):\tLoss 0.1887\tAccurary 93.83%\t\tTime 8.21s\n",
      "01/21 07:14:25 PM | Epoch[1] (43776/50000):\tLoss 0.1887\tAccurary 93.83%\t\tTime 8.21s\n",
      "01/21 07:14:25 PM | Epoch[1] (43776/50000):\tLoss 0.1887\tAccurary 93.83%\t\tTime 8.21s\n",
      "01/21 07:14:33 PM | Epoch[1] (48640/50000):\tLoss 0.1919\tAccurary 93.76%\t\tTime 8.04s\n",
      "01/21 07:14:33 PM | Epoch[1] (48640/50000):\tLoss 0.1919\tAccurary 93.76%\t\tTime 8.04s\n",
      "01/21 07:14:33 PM | Epoch[1] (48640/50000):\tLoss 0.1919\tAccurary 93.76%\t\tTime 8.04s\n",
      "01/21 07:14:33 PM | Epoch[1] (48640/50000):\tLoss 0.1919\tAccurary 93.76%\t\tTime 8.04s\n",
      "01/21 07:14:33 PM | Epoch[1] (48640/50000):\tLoss 0.1919\tAccurary 93.76%\t\tTime 8.04s\n",
      "01/21 07:14:41 PM | Test Loss 0.4612\tAccurary 85.67%\t\tTime 5.23s\n",
      "\n",
      "01/21 07:14:41 PM | Test Loss 0.4612\tAccurary 85.67%\t\tTime 5.23s\n",
      "\n",
      "01/21 07:14:41 PM | Test Loss 0.4612\tAccurary 85.67%\t\tTime 5.23s\n",
      "\n",
      "01/21 07:14:41 PM | Test Loss 0.4612\tAccurary 85.67%\t\tTime 5.23s\n",
      "\n",
      "01/21 07:14:41 PM | Test Loss 0.4612\tAccurary 85.67%\t\tTime 5.23s\n",
      "\n",
      "01/21 07:14:41 PM | ############# model6_2epoch:85.66999816894531 using 184.37777876853943s #################\n",
      "01/21 07:14:41 PM | ############# model6_2epoch:85.66999816894531 using 184.37777876853943s #################\n",
      "01/21 07:14:41 PM | ############# model6_2epoch:85.66999816894531 using 184.37777876853943s #################\n",
      "01/21 07:14:41 PM | ############# model6_2epoch:85.66999816894531 using 184.37777876853943s #################\n",
      "01/21 07:14:41 PM | ############# model6_2epoch:85.66999816894531 using 184.37777876853943s #################\n"
     ]
    }
   ],
   "source": [
    "acc1_bf = [29.58,0.2946,0.2874,0.2834,0.2074,0.1856,0.1224]\n",
    "action = [\n",
    "    [0.75, 0.75, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.71875, 0.734375, 0.734375, 0.75, 0.5625],\n",
    "    [0.75, 0.75, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.71875, 0.71875, 0.734375, 0.75, 0.640625],\n",
    "    [0.75, 0.75, 0.6875, 0.6875, 0.6875, 0.6875, 0.65625, 0.71875, 0.71875, 0.734375, 0.734375, 0.765625, 0.53125],\n",
    "    [0.625, 0.75, 0.75, 0.8125, 0.65625, 0.53125, 0.625, 0.71875, 0.78125, 0.796875, 0.609375, 0.875, 0.671875],\n",
    "    [0.75, 1.0, 0.625, 0.8125, 0.8125, 0.53125, 0.78125, 0.625, 0.8125, 0.421875, 0.625, 0.734375, 0.46875],\n",
    "    [0.875, 0.75, 0.9375, 0.8125, 0.9375, 0.65625, 0.28125, 0.890625, 0.21875, 0.75, 0.5, 0.203125, 0.5],\n",
    "    [0.75, 0.375, 0.8125, 1.0, 0.8125, 0.53125, 0.71875, 0.875, 0.296875, 0.59375, 0.921875, 0.265625, 0.203125]\n",
    "]\n",
    "T = 2\n",
    "ft_acc = []\n",
    "logger = get_logger('./log.log')\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "for idx,ai in enumerate(action):\n",
    "    model = get_masked_vgg(ai)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-3)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2], gamma=0.1)\n",
    "#     print(f'############# model{idx} #################')\n",
    "    best_acc = 0.0\n",
    "    st = time.time()\n",
    "    for epoch in range(0, T):\n",
    "        train(model, optimizer, loader.trainLoader, args, epoch)\n",
    "        scheduler.step()\n",
    "        test_acc = test(model, loader.testLoader)\n",
    "        is_best = best_acc < test_acc\n",
    "        best_acc = max(best_acc, test_acc)\n",
    "    ed = time.time()\n",
    "    logger.info(f\"############# model{idx}_{T}epoch ft-acc:{best_acc} using {ed-st}s #################\")\n",
    "    ft_acc.append(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(87.0900, device='cuda:0'),\n",
       " tensor(87.9600, device='cuda:0'),\n",
       " tensor(88.3100, device='cuda:0'),\n",
       " tensor(88.8200, device='cuda:0'),\n",
       " tensor(88.8400, device='cuda:0'),\n",
       " tensor(87.5800, device='cuda:0'),\n",
       " tensor(85.6700, device='cuda:0')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29.58, 0.2946, 0.2874, 0.2834, 0.2074, 0.1856, 0.1224]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1_bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.function_base import select\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "from models.vgg_cifar import MaskVGG,VGG\n",
    "\n",
    "\n",
    "class Pruning:\n",
    "    def __init__(self,origin_model) -> None:\n",
    "        super().__init__()\n",
    "        self.origin_model = copy.deepcopy(origin_model)\n",
    "        self.get_layer_information()\n",
    "\n",
    "    def get_masked(self,select):\n",
    "        self.all_mask = self.preprocess_get_mask(select)\n",
    "        masked_vgg = MaskVGG('vgg16',select).cuda()\n",
    "        self.pruned_model(masked_vgg)\n",
    "        return masked_vgg\n",
    "\n",
    "    def get_layer_information(self):\n",
    "        self.prunable_idx = []\n",
    "        self.prunable_ops = []\n",
    "        self.layer_type_dict = {}\n",
    "        self.orgin_channel = []\n",
    "        self.conv_buffer_dict = {} # layer after the conv\n",
    "        self.all_idx = []\n",
    "        self.buffer_conv_map = {}\n",
    "\n",
    "        i=0\n",
    "        buffer_temp_idx = []\n",
    "        model = self.origin_model\n",
    "        modules = list(model.modules())\n",
    "        n = len(modules)\n",
    "        while i < n :\n",
    "            m = modules[i]\n",
    "\n",
    "            if type(m) not in [nn.Conv2d,nn.BatchNorm2d,nn.Linear,nn.AvgPool2d,nn.MaxPool2d,nn.ReLU]:\n",
    "                i+=1\n",
    "                continue\n",
    "            else:\n",
    "                assert type(m) == torch.nn.modules.conv.Conv2d\n",
    "                self.prunable_ops.append(m)\n",
    "                self.prunable_idx.append(i)\n",
    "                self.layer_type_dict[i] = type(m)\n",
    "                self.orgin_channel.append(m.out_channels) \n",
    "                self.all_idx.append(i)\n",
    "                buffer_temp_idx = []\n",
    "                while i != n-1:\n",
    "                    i+=1\n",
    "                    bu = modules[i]\n",
    "                    if type(bu) is torch.nn.modules.conv.Conv2d:\n",
    "                        i-=1\n",
    "                        break\n",
    "                    buffer_temp_idx.append(i)\n",
    "                    self.all_idx.append(i)\n",
    "                self.conv_buffer_dict[self.prunable_idx[-1]] = copy.deepcopy(buffer_temp_idx)\n",
    "                for j in buffer_temp_idx:\n",
    "                    self.buffer_conv_map[j] = self.prunable_idx[-1]\n",
    "            i+=1\n",
    "\n",
    "    # def vgg_masked(self,strategy):\n",
    "    #     masked_vgg = MaskVGG('vgg16',strategy)\n",
    "    #     orimo_ls = list(self.orgin_model.modules())\n",
    "        \n",
    "    #     for i,m in enumerate(masked_vgg.modules()):\n",
    "    #         if type(m) in [nn.Conv2d,nn.BatchNorm2d,nn.Linear,nn.AvgPool2d,nn.MaxPool2d,nn.ReLU]:\n",
    "    #             # type\n",
    "    #             ty = type(m)\n",
    "\n",
    "    #             # conv\n",
    "    #             if ty == nn.Conv2d:\n",
    "    #                 m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "    #                 m.bias.data.copy_(orimo_ls[i].bias.data)\n",
    "    #             # bn2d\n",
    "    #             elif ty == nn.BatchNorm2d:\n",
    "    #                 m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "    #                 m.bias.data.copy_(orimo_ls[i].bias.data)\n",
    "    #                 m.running_mean.data.copy_(orimo_ls[i].running_mean.data)\n",
    "    #                 m.running_var.data.copy_(orimo_ls[i].running_var.data)\n",
    "    #             elif ty == nn.Linear:\n",
    "    #                 # linear\n",
    "    #                 m.weight.data.copy_(orimo_ls[i].weight.data)\n",
    "    #             else:# maxpool,avgpool,relu don't need params\n",
    "    #                 pass\n",
    "    #     masked_vgg = masked_vgg.cuda()\n",
    "    #     return masked_vgg\n",
    "\n",
    "\n",
    "    def pruned_model(self,masked_vgg):\n",
    "        m_list = list(self.origin_model.modules())\n",
    "        mp_list = list(masked_vgg.modules())\n",
    "        for idx,idxx in enumerate(self.prunable_idx):\n",
    "            \n",
    "            # replace conv first\n",
    "            mask = self.all_mask[idx]\n",
    "            weight = m_list[idxx].weight.data.cpu().numpy()\n",
    "            bias = m_list[idxx].bias.data.cpu().numpy()\n",
    "            \n",
    "            mask_weight = None\n",
    "            if idx == 0:\n",
    "                mask_weight = weight[mask,:,:,:]\n",
    "            else:\n",
    "                input_mask = self.all_mask[idx-1]\n",
    "                # select input\n",
    "                mask_weight = weight[:,input_mask,:,:].reshape(weight.shape[0],-1,weight.shape[2],weight.shape[3])\n",
    "                # select output\n",
    "                mask_weight = mask_weight[mask,:,:,:].reshape(-1,mask_weight.shape[1],mask_weight.shape[2],mask_weight.shape[3])\n",
    "            mask_bias = bias[mask]\n",
    "            mp = mp_list[idxx]\n",
    "            mp.weight.data.copy_(torch.from_numpy(mask_weight).cuda())\n",
    "            mp.bias.data.copy_(torch.from_numpy(mask_bias).cuda())\n",
    "\n",
    "            # replace other layers\n",
    "            buffer = self.conv_buffer_dict[idxx]\n",
    "            for buffer_idx in buffer:\n",
    "                m = m_list[buffer_idx]\n",
    "                mp = mp_list[buffer_idx]\n",
    "                if type(m) == nn.BatchNorm2d:\n",
    "                    mp.weight.data.copy_(torch.from_numpy(m.weight.data.cpu().numpy()[mask]).cuda())\n",
    "                    mp.bias.data.copy_(torch.from_numpy(m.bias.data.cpu().numpy()[mask]).cuda())\n",
    "                    mp.running_mean.data.copy_(torch.from_numpy(m.running_mean.data.cpu().numpy()[mask]).cuda())\n",
    "                    mp.running_var.data.copy_(torch.from_numpy(m.running_var.data.cpu().numpy()[mask]).cuda())\n",
    "                elif type(m) == nn.Linear:\n",
    "                    mp.weight.data.copy_(torch.from_numpy(m.weight.data.cpu().numpy()[:,mask]).cuda())\n",
    "    #     print(f'replace cost {ed-st}s')\n",
    "    def preprocess_get_mask(self,select,method = 'l1'):\n",
    "        mask = []\n",
    "        for i,a in enumerate(select):\n",
    "            c = self.orgin_channel[i]\n",
    "            d = int(c * a)\n",
    "            mask_ = np.zeros(c,bool)\n",
    "            weight = self.prunable_ops[i].weight.data.cpu().numpy()\n",
    "            if method == 'l1':\n",
    "                importance = np.abs(weight).sum((1, 2, 3))\n",
    "                sorted_idx = np.argsort(-importance)  # sum magnitude along C_in, sort descend\n",
    "                preserve_idx = sorted_idx[:d]  # to preserve index\n",
    "                mask_[preserve_idx] = True\n",
    "            mask.append(mask_)\n",
    "        return mask\n",
    "\n",
    "    def idx2idxx(self,idx):\n",
    "        return self.prunable_idx[idx]\n",
    "\n",
    "    def idxx2idx(self,idxx):\n",
    "        return self.prunable_idx.index(idxx)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     origin_model = VGG('vgg16')\n",
    "#     pruner = Pruning(origin_model)\n",
    "#     select=[1.0]*13\n",
    "#     get_acc(select)\n",
    "# #     pruned_model(vgg,masked_vgg,mask)\n",
    "# #     mask_model = pruner.get_masked(select=select)\n",
    "# #     _validate(val_loader,mask_model)\n",
    "# #     print(mask_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10_Valider:\n",
    "    def __init__(self):\n",
    "        self.val_loader = self.get_split_dataset()\n",
    "        \n",
    "    def validate(self, model, verbose=True):\n",
    "        '''\n",
    "        Validate the performance on validation set\n",
    "        :param val_loader:\n",
    "        :param model:\n",
    "        :param verbose:\n",
    "        :return:\n",
    "        '''\n",
    "        val_loader=self.val_loader\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        top5 = AverageMeter()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        # switch to evaluate mode\n",
    "        model.eval()\n",
    "        end = time.time()\n",
    "\n",
    "        t1 = time.time()\n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(val_loader):\n",
    "                target = target.cuda(non_blocking=True)\n",
    "                input_var = torch.autograd.Variable(input).cuda()\n",
    "                target_var = torch.autograd.Variable(target).cuda()\n",
    "\n",
    "                # compute output\n",
    "                output = model(input_var)\n",
    "                loss = criterion(output, target_var)\n",
    "\n",
    "                # measure accuracy and record loss\n",
    "                prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "                losses.update(loss.item(), input.size(0))\n",
    "                top1.update(prec1.item(), input.size(0))\n",
    "                top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "                # measure elapsed time\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "        t2 = time.time()\n",
    "        if verbose:\n",
    "            print('* Test loss: %.3f    top1: %.3f    top5: %.3f    time: %.3f' %\n",
    "                  (losses.avg, top1.avg, top5.avg, t2 - t1))\n",
    "    def get_split_dataset(self, batch_size=128, n_worker=0, val_size=5000, data_root='C:\\\\Users\\\\lenovo\\\\dataset\\\\cifar',\\\n",
    "                      shuffle=True):\n",
    "        '''\n",
    "            split the train set into train / val for rl search\n",
    "        '''\n",
    "        if shuffle:\n",
    "            index_sampler = SubsetRandomSampler\n",
    "        else:  # every time we use the same order for the split subset\n",
    "            class SubsetSequentialSampler(SubsetRandomSampler):\n",
    "                def __iter__(self):\n",
    "                    return (self.indices[i] for i in torch.arange(len(self.indices)).int())\n",
    "            index_sampler = SubsetSequentialSampler\n",
    "\n",
    "#         print('=> Preparing data: {}...'.format(dset_name))\n",
    "        \n",
    "#         transform_train = transforms.Compose([\n",
    "#             transforms.RandomCrop(32, padding=4),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "#         ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "#         trainset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_train)\n",
    "\n",
    "        valset = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform_test)\n",
    "        n_val = len(valset)\n",
    "        assert val_size < n_val\n",
    "        indices = list(range(n_val))\n",
    "        np.random.shuffle(indices)\n",
    "        _, val_idx = indices[val_size:], indices[:val_size]\n",
    "#         train_idx = list(range(len(trainset)))  # all train set for train\n",
    "\n",
    "#         train_sampler = index_sampler(train_idx)\n",
    "        val_sampler = index_sampler(val_idx)\n",
    "\n",
    "#         train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, sampler=train_sampler,\n",
    "#                                                    num_workers=n_worker, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, sampler=val_sampler,\n",
    "                                                 num_workers=n_worker, pin_memory=True)\n",
    "        n_class = 10\n",
    "\n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cv = Cifar10_Valider()\n",
    "# cv.validate(get_masked_vgg([1.0]*13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = preprocess_get_mask(select)\n",
    "# from models.vgg_cifar import MaskVGG\n",
    "# masked_vgg = MaskVGG('vgg16',select).cuda()\n",
    "# _validate(val_loader,masked_vgg)\n",
    "# pruned_model(vgg,masked_vgg,mask)\n",
    "# _validate(val_loader,masked_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_rate(action):\n",
    "    macs, params = get_fp_from_action(action)\n",
    "    macs_orginal, params_orginal = get_fp_from_action([1.0]*13)\n",
    "    print('preserve:')\n",
    "    print(macs/macs_orginal,params/params_orginal)\n",
    "    print('pruned:')\n",
    "    print(1-macs/macs_orginal,1-params/params_orginal)\n",
    "#     return macs/macs_orginal,params/params_orginal\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops_params_from_model(model):\n",
    "    from thop import profile\n",
    "    import torch\n",
    "    input = torch.randn(1, 3, 32, 32)\n",
    "    macs, params = profile(model, inputs=(input, ))\n",
    "#     print(macs/1e6, params/1e6)\n",
    "    return macs/1e6, params/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fp_from_action(action):\n",
    "    mask = preprocess_get_mask(action)\n",
    "    from models.vgg_cifar import MaskVGG\n",
    "    masked_vgg = MaskVGG('vgg16',action)\n",
    "#     print()\n",
    "    return get_flops_params_from_model(masked_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = [0.75, 0.625, 1.0, 0.25, 0.90625, 0.34375, 0.90625, 0.421875, 0.625, 0.203125, 0.765625, 0.203125, 0.40625]\n",
    "# get_fp_from_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = [0.75, 0.625, 1.0, 0.25, 0.90625, 0.34375, 0.90625, 0.421875, 0.625, 0.203125, 0.765625, 0.203125, 0.40625]\n",
    "# get_reduced_rate(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_acc([0.25,0.5,0.5625,0.1875,0.9375,0.25,0.15625,0.109375,0.109375,0.28125,0.109375,0.109375,0.703125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = torch.load('./logs/vgg16_cifar10_finetune-run56/ckpt.best.pth.tar')\n",
    "# vgg16_30 = MaskVGG('vgg16',[0.75, 0.625, 1.0, 0.25, 0.90625, 0.34375, 0.90625, 0.421875, 0.625, 0.203125, 0.765625, 0.203125, 0.40625])\n",
    "# vgg16_30.load_state_dict(dic['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.choice(range(3),1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "m = torch.nn.Conv2d(in_channels=3,out_channels=1,kernel_size=3,padding=1,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1574,  0.1919, -0.0597],\n",
       "          [ 0.0149,  0.1871, -0.0072],\n",
       "          [ 0.0056,  0.0513, -0.0983]],\n",
       "\n",
       "         [[-0.0168, -0.1040, -0.1004],\n",
       "          [-0.0662,  0.1262, -0.0367],\n",
       "          [-0.0690,  0.1048, -0.0518]],\n",
       "\n",
       "         [[-0.0606,  0.0876,  0.0088],\n",
       "          [-0.0724,  0.0696,  0.1688],\n",
       "          [ 0.1339, -0.0232,  0.0223]]]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((50,3,2,2))\n",
    "# input\n",
    "input_pad = torch.nn.functional.pad(input,pad=(1,1,1,1),value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Conv2d(in_channels=3,out_channels=1,kernel_size=3,padding=1,bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.2503,  0.1073,  0.1506, -0.4082,  0.1296, -0.4880, -0.0871, -0.6540,\n",
       "        -0.3553,  0.0674,  0.3267, -0.2391, -0.0998,  0.9294,  0.0316, -0.1539,\n",
       "         0.2457,  0.8251, -0.2070,  0.6967, -0.7093,  1.0018, -0.0552, -0.1139,\n",
       "         0.1567,  0.1967, -1.2273, -0.2346,  0.5532, -0.3946, -0.1133, -0.9122,\n",
       "        -0.3404, -0.3401, -0.6173,  0.3668, -0.6848, -0.2288, -0.0215,  0.1898,\n",
       "        -0.2610,  0.4238,  0.2819, -0.7106, -0.3907, -0.7498,  0.1644,  0.4771,\n",
       "        -0.0038,  0.3640], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "torch.sum(input_pad[:,:,0:3,0:3]  * m.weight,dim=(3,2,1))+ m.bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-2.0289, grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "\n",
    "torch.sum(input_pad[:,:,0:3,0:3]  * m.weight)+ m.bias.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.3911]], grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "y[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "input_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0.5848, 0.4401],\n",
       "          [0.1162, 0.4401]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "y = m(input)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4401]], grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "y[:,:,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "input_pad[:,:,:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "m.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-0.0238, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load(r'C:\\Users\\lenovo\\Desktop\\cacp\\cacp_vgg\\checkpoints\\vgg16_cifar10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3,64, 64, 'M', 64, 128, 'M', 128, 256, 256, 'M', 256, 512, 512, 'M', 512, 512, 512, 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "len([1.0,0.3,0.3,0.3,0.3,0.3,0.3,0.1,0.2,0.3,0.3,0.3,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.3530,  0.2482,  0.4256], grad_fn=<SumBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "m.weight.sum((0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nn.Linear(512,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "B.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "m = torch.nn.Conv2d(in_channels=3,out_channels=4,kernel_size=3,padding=1,bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = np.zeros(4,bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "m.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearRegression(fit_intercept=False)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "X = torch.randn((200,27))\n",
    "Y = torch.randn((200,10))\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[[ 3.44062261e-02, -2.53497902e-03, -4.46233824e-02],\n",
       "         [ 2.72693541e-02, -2.63566840e-02, -1.00391440e-01],\n",
       "         [-1.66097984e-01,  7.32494742e-02, -4.23325300e-02]],\n",
       "\n",
       "        [[-2.20595021e-02, -6.38243034e-02,  1.85469389e-02],\n",
       "         [-5.87097090e-03,  6.67332709e-02,  7.68231750e-02],\n",
       "         [ 9.88645628e-02,  1.20755650e-01, -2.03198493e-02]],\n",
       "\n",
       "        [[ 9.13487934e-03, -4.31717336e-02,  4.07561511e-02],\n",
       "         [ 6.15837350e-02,  6.99409619e-02, -5.98619059e-02],\n",
       "         [ 1.43059805e-01, -1.03198156e-01, -9.30793397e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 1.27959207e-01,  8.28858465e-03, -2.31272236e-01],\n",
       "         [ 7.38568678e-02, -2.65119337e-02, -5.69740459e-02],\n",
       "         [ 3.60761657e-02,  2.94754300e-02,  9.32743549e-02]],\n",
       "\n",
       "        [[ 1.87041193e-01, -1.05630651e-01, -5.53176692e-03],\n",
       "         [-1.98659282e-02, -3.49560343e-02,  5.87630868e-02],\n",
       "         [ 9.93013233e-02,  6.64867088e-02,  7.84636587e-02]],\n",
       "\n",
       "        [[-2.58530471e-02,  6.37312829e-02, -7.40159601e-02],\n",
       "         [ 7.79157504e-02, -2.46757530e-02, -8.44062418e-02],\n",
       "         [-4.40092012e-03, -1.36570483e-01, -8.36318638e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 1.41944706e-01,  8.52455050e-02, -8.69520083e-02],\n",
       "         [ 1.45618105e-02,  1.30460680e-01,  1.12270797e-02],\n",
       "         [-5.45067117e-02,  9.82855707e-02,  2.16460284e-02]],\n",
       "\n",
       "        [[-7.64783099e-02,  8.65116417e-02, -7.55481645e-02],\n",
       "         [ 1.33080855e-01,  1.24240205e-01, -4.59995642e-02],\n",
       "         [ 1.33626580e-01,  6.69175088e-02, -3.33374478e-02]],\n",
       "\n",
       "        [[-6.18165955e-02,  5.99609874e-02,  1.77029535e-01],\n",
       "         [ 8.10004398e-02,  5.31708635e-02,  5.24227992e-02],\n",
       "         [ 5.52842245e-02,  5.23889288e-02, -4.35532108e-02]]],\n",
       "\n",
       "\n",
       "       [[[-3.15046199e-02, -5.52665815e-02,  1.06114671e-02],\n",
       "         [ 1.43347494e-02,  4.95750085e-02,  1.13477059e-01],\n",
       "         [-5.71457706e-02, -9.25707221e-02,  1.02165751e-02]],\n",
       "\n",
       "        [[ 3.92431393e-03, -9.29884911e-02, -8.42008367e-02],\n",
       "         [ 1.22524641e-01,  1.98459588e-02, -5.85315526e-02],\n",
       "         [ 9.53064412e-02, -1.83410361e-01, -5.84247708e-03]],\n",
       "\n",
       "        [[-1.21725827e-01,  7.01109916e-02,  2.66096592e-02],\n",
       "         [ 1.01874433e-02,  3.33785303e-02, -1.15604647e-01],\n",
       "         [-7.00877607e-03,  1.67343430e-02, -1.20003246e-01]]],\n",
       "\n",
       "\n",
       "       [[[-8.39291885e-02, -7.23107755e-02,  4.74550091e-02],\n",
       "         [ 1.37917832e-01,  4.21686471e-02,  7.03127384e-02],\n",
       "         [-4.72821258e-02,  1.80899441e-01, -1.10988565e-01]],\n",
       "\n",
       "        [[ 2.34606843e-02,  7.10458905e-02,  6.58828914e-02],\n",
       "         [ 6.72824681e-02, -5.58552779e-02,  2.55983528e-02],\n",
       "         [-6.12476654e-02, -6.91878200e-02,  2.40445845e-02]],\n",
       "\n",
       "        [[ 1.39815107e-01,  1.56507432e-01, -1.15708932e-02],\n",
       "         [-5.80035374e-02, -1.04858428e-01,  5.28996140e-02],\n",
       "         [-1.42287873e-02, -8.54717046e-02,  4.83411998e-02]]],\n",
       "\n",
       "\n",
       "       [[[-5.66968461e-03,  1.12261958e-01, -6.83990046e-02],\n",
       "         [ 1.91345140e-02, -7.15504885e-02,  6.03032485e-02],\n",
       "         [ 7.19472170e-02, -3.76477838e-05,  1.18876129e-01]],\n",
       "\n",
       "        [[-9.43772774e-03,  2.17921920e-02, -3.90365310e-02],\n",
       "         [ 1.74699761e-02,  6.43540099e-02,  3.83762941e-02],\n",
       "         [ 2.39494033e-02, -4.46377844e-02, -1.55577213e-01]],\n",
       "\n",
       "        [[-2.00875383e-02, -1.33981630e-02,  9.10195243e-03],\n",
       "         [-2.34017204e-02, -1.59286857e-01,  2.22733412e-02],\n",
       "         [ 6.59024864e-02,  1.23369791e-01,  2.69282199e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 6.32923022e-02, -1.03703842e-01, -2.27800012e-03],\n",
       "         [ 1.52675927e-01,  7.86859095e-02, -5.60946576e-03],\n",
       "         [-1.35499015e-01, -6.73384815e-02, -5.18354177e-02]],\n",
       "\n",
       "        [[ 1.44753322e-01,  5.62981889e-02,  2.71947719e-02],\n",
       "         [-4.66143386e-03, -6.58366084e-02,  7.55975395e-03],\n",
       "         [-1.19148672e-01,  2.23657768e-02,  9.38422047e-04]],\n",
       "\n",
       "        [[-6.69554807e-03,  2.47500464e-02, -2.10350845e-02],\n",
       "         [-1.28814969e-02, -6.15565479e-02,  6.84356689e-02],\n",
       "         [ 6.75337538e-02, -8.57779682e-02, -2.19000876e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 1.16919642e-02, -1.02567136e-01, -1.40820509e-02],\n",
       "         [ 2.86643989e-02, -5.25632501e-02,  3.62651646e-02],\n",
       "         [-3.13979201e-02,  1.48040708e-03, -1.79296397e-02]],\n",
       "\n",
       "        [[-6.90786634e-03, -5.76482154e-02, -1.56306848e-02],\n",
       "         [ 1.11286469e-01,  1.51632085e-01, -2.89781168e-02],\n",
       "         [-9.81278196e-02,  1.49545483e-02,  8.67912993e-02]],\n",
       "\n",
       "        [[-1.03373826e-02,  3.50447148e-02, -7.44299144e-02],\n",
       "         [-2.46152119e-03, -1.85635667e-02, -1.39665440e-01],\n",
       "         [-1.00217670e-01, -8.29813927e-02,  9.97786596e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 8.41071084e-02,  1.51225388e-01,  6.07947558e-02],\n",
       "         [ 2.57779378e-02,  1.12083867e-01,  2.68768109e-02],\n",
       "         [-4.90845041e-03, -5.49812056e-03, -1.28323615e-01]],\n",
       "\n",
       "        [[-3.71471010e-02,  3.53027210e-02, -5.53722158e-02],\n",
       "         [ 1.28400132e-01,  5.32102883e-02,  9.40312743e-02],\n",
       "         [-2.38885172e-02, -4.66770828e-02, -1.46996796e-01]],\n",
       "\n",
       "        [[ 2.38742158e-02,  6.16732910e-02, -8.09757710e-02],\n",
       "         [ 3.89220007e-02, -1.84134953e-02, -1.31112248e-01],\n",
       "         [-7.12601393e-02,  9.23480652e-03, -8.32686275e-02]]],\n",
       "\n",
       "\n",
       "       [[[-2.23905072e-02,  5.87762371e-02, -5.18883690e-02],\n",
       "         [-1.09473698e-01, -9.60171968e-02,  1.32337678e-02],\n",
       "         [ 3.84529121e-02, -1.21581137e-01,  6.05344400e-02]],\n",
       "\n",
       "        [[ 3.30092683e-02, -9.37824398e-02, -4.71090935e-02],\n",
       "         [-8.86906087e-02, -4.75063026e-02, -5.47036827e-02],\n",
       "         [-1.23161450e-03, -5.59203140e-03,  1.35685652e-01]],\n",
       "\n",
       "        [[-3.32529731e-02, -4.70683724e-02, -1.28615648e-01],\n",
       "         [-1.06830433e-01,  4.94166538e-02,  4.18545157e-02],\n",
       "         [ 3.32669243e-02,  9.71462131e-02, -3.12325172e-02]]]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "W.reshape((10,3,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}